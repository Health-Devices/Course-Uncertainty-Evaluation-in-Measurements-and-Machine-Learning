{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local and global sensitivity analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "+ Defination of Local and Global Sensitivity Analysis\n",
    "+ ANOVA(analysis of variance) Decomposition\n",
    "+ Sobol indices\n",
    "+ some functions\n",
    "+ Morris sensitivity analysis \n",
    "+ sensitivity analysis in metrology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Symbols\n",
    "D output variance\n",
    "$S_i$ sensitivity indices\n",
    "$S_{Ti}$ total sensitivity indices\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definitions\n",
    "\n",
    "### Local Sensitivity Analysis\n",
    "local sensitivity analysis foucs on how the small perturbation near an input xi will influence Y output of the model.A common apprach is to change only one factor each time ,and detect the influence of this fator on the output,and repeat this process until all the factors are detected,this method is also called OAT(One factor At Time)method.\n",
    "### Global Sensitivity Analysis\n",
    "Global sensitivity focus on the influence how changes of multiple inputs affect the output of the model,and it enables to detect how the interaction between multiple inputs affect the outputs.\n",
    "### The goal of Global Sensitivity Analysis\n",
    "+ if we can rank how all the model factors conritbute to the output of the model,we can better understand the model outputs and and find out the most effective way to improve our model.\n",
    "+ it also enables to partition output variance by detecting the factors of the model,so that we can find the main value,main effects  and all the interactions\n",
    "<img src=\"goal.PNG\",width = 350,height = 350>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANOVA(analysis of variance) Decomposition\n",
    "+ High-Dimensional Model Representation of f(x) [4]\n",
    " $f(x)=f0+\\sum_{i}f_i(x_i)+\\sum_{i<j}f_{(ij)}(x_i,x_j)+...+f_{12...n}(x_1,x_2,...,x_n)$\n",
    " \n",
    " the model is partition into several parts,fo is the mean value,$\\sum_{i}f_i(x_i)$ represents the sum of the single variable $x_i$,and is also called main effects, $\\sum_{i<j}f_{(ij)}$ is the sum of bases which involve two variable interaction,this part is called First-order interactions,and so on.\n",
    "   \n",
    " This is an example HDMR for a function of three parameters\n",
    "$f(x)=f0+f_1(x_1)+f_2(x_2)+f_3(x_3)+f_{(12)}(x_1,x_2)+f_{(13)}(x_1,x_3)+f_{(23)}(x_2,x_3)+f_{(123)}(x_1,x_2,x_3)$\n",
    "\n",
    " we can see in this function,$f_0$ is the mean value,and $f_1(x_1)+f_2(x_2)+f_3(x_3)$ is the main effects part,if want to improve this model we can foucs on this part,and then $f_{(12)}(x_1,x_2)+f_{(13)}(x_1,x_3)+f_{(23)}(x_2,x_3)$ is the First-order interactions,and $f_{(123)}(x_1,x_2,x_3)$ is the Second-order interaction.\n",
    "+ Calculate the ANOVA decompostion\n",
    "  + the main value of the model is to calculate the expectation of the model\n",
    "$$f_0=E[f]$$\n",
    "  + the main effect part is calculated by expectation of the output of single variable minus the main value\n",
    " $$f_i=E[f\\mid x_i]-f_0$$\n",
    "  + the two variables interaction is calculated by expectation of the output of two varibales minus the main effect parts and main value part\n",
    " $$f_{i,j}=E[f\\mid x_i,x_j]-\\sum_{i} f_i-f_0$$\n",
    "  + from the previous fuction we can get the funtion of mutiple variables interactions\n",
    " $$f_{1,2,3,...,m}=E[f(x)]-'everything else'$$\n",
    "\n",
    "Here is a example for calculating the ANOVA decompostion \n",
    "\n",
    "Given a model:$y=ax_1+bx_2^2+cx_1x_2+d$\n",
    "\n",
    " $f_0=\\int_0^1\\int_0^1 ydx_1dx_2=\\int_0^1\\frac{a}{2}+bx_2^2+\\frac{c}{2}x_2+d dx_2=\\frac{a}{2}+\\frac{b}{3}+\\frac{c}{4}+d$\n",
    "\n",
    " $f_1(x_1)=\\int_0^1ydx_2-f_0=ax_1+\\frac{b}{3}+\\frac{c}{2}x_1+d-f_0=a(x_1-\\frac{1}{2})+c(\\frac{x_1}{2}-\\frac{1}{4})$\n",
    "\n",
    " $f_2(x_2)=\\int_0^1ydx_1-f_0=\\frac{a}{2}+bx_2^2+\\frac{c}{2}x_2+d-f_0=b(x_2^2-\\frac{1}{3})+c(\\frac{x_2}{2}-\\frac{1}{4})$\n",
    "\n",
    " $f_{12}(x_1,x_2)=y-f_1-f_2-f_0=c(x_1x_2-\\frac{x_1}{2}-\\frac{x_2}{2}+\\frac{1}{4})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sobal indices\n",
    "Variance-based sensitivity analysis (often referred to as the Sobol method or Sobol indices, after Ilya M. Sobol) is a form of global sensitivity analysis[1].\n",
    "### Calculating Variances\n",
    "to calculate the Sobol indices,we should first calculate the Varances.\n",
    "+ Given a function f(x) that is square integrable, \n",
    "  Variance of f can be represented as:\n",
    "  $$V=\\int{f(x)^2dx-f_0^2}=D$$\n",
    "+ Similarly we can get Variance of $f_i$,\n",
    "  $$V_i=\\int_0^1{f_i(x_i)^2dx_i}=D_i$$\n",
    "+ we can also get the Variance of $f_{ij}$:\n",
    "  $$Var_{ij}=Var[E[f\\mid x_i,x_j]]-V_i-V_i=D_{ij}$$\n",
    "+ so the Variance of model Y:\n",
    "  $$Var(Y)=\\sum_{i=1}^{d}V_i+\\sum_{i<j}^{d}V_{ij}+...+V_{12...d}$$\n",
    "### Monte Carlo Estimates\n",
    "in the Variance calculating part,we need the expectation of f(x),we use Monte Carlo method to get E[f(x)]\n",
    "+ The expected value of f(x) may be estimated from:\n",
    "$$\\hat{f_0}=\\frac{1}{N} \\sum_{m=1}^{N} f(\\bar{x}_m)$$\n",
    "+ The variance of f(x) may then be estimated from:\n",
    "$$\\hat{D}=\\frac{1}{N} \\sum_{m=1}^{N} f(\\bar{x}_m)^2-\\hat{f_0}^2$$\n",
    "+ For the one indexed terms the variance may be estimated from:\n",
    "$$\\hat{D_i}=\\frac{1}{N} \\sum_{m=1}^{N} f(\\bar{u}_m,x_i)f(\\bar{v}_m,x_i)-\\hat{f_0}^2$$\n",
    "### First-order Sensitivity  indices\n",
    "the First-order Sensitivity indices is a direct variance-based measure of sensitivity Si can be calculated by:\n",
    "$$S_i=\\frac{V_i}{Var(Y)}$$\n",
    "+ The main (or first-order) effect (Si) measures the contribution to the output variance from varying the i-the factor alone (but averaged over variations in other factors)\n",
    "\n",
    "(i) the higher the value of Si, the higher the influence of the i-th factor on the output\n",
    "\n",
    "(i) if Si = 0, then the i-th parameter has no direct influence on the output (but it might still have some in interaction with other parameters!)\n",
    "\n",
    "(iii) the sum of all Si is always lower or equal to 1. If it is equal to 1, then there are no interactions between the parameters (“additive” model).\n",
    "### Total Sensitivity Indices\n",
    "+ Sensitivity indices:\n",
    "$$\\hat S_i=\\frac{\\hat{D_i}}{\\hat{D}},\\sum_{S} \\hat{S_k}=1$$\n",
    "+ Total Sensitivity indices:\n",
    "$$\\hat S_{Ti}=1-\\hat S_{i^c}=1-\\frac{\\hat{D_{i^c}}}{\\hat{D}}, \\sum_{i} \\hat{S_{Ti}}\\geq1$$\n",
    "+ The total effect (STi)\tmeasures the total contribution to the output variance of the i-th factor, including its direct effect and interactions with other factors\n",
    "\n",
    "(i)  STi must be higher or equal to Si. If it is equal, then the parameter has no interactions with the other parameters\n",
    "\n",
    "(ii)  if STi = 0, the i-th parameter has no influence (neither direct or indirect) on the model output\n",
    "\n",
    "(ii) the sum of all STi is always higher or equal to 1. If it is equal to 1, then there are no interactions between the parameters\n",
    " ### Sensitivity indices - Advantages and Disadvantages\n",
    "+ Advantages\n",
    "Extremely robust, they work with any type of discontinuous (even randomised) mapping between input factors and the output. Sobol’ estimator is unbiased. They do not rely on any hypothesis about the smoothness of the mapping. The only key assumption is that variance (i.e. the second moment) is an adequate measure for quantifying the uncertainty of the model output.\n",
    "Computing main effects and total effects for each factor, while still being far from a full factors mapping, gives a fairly instructive description of the system. Moreover, they provide unambiguous and clear answers to well specified sensitivity settings (prioritisation and fixing).\n",
    "+ Disadvantages\n",
    "The computational cost is relatively high, which implies that these methods cannot be applied to computationally expensive models. They do not provide any mapping, i.e. they decompose the output uncertainty but they do not provide information about, e.g., the input factors responsible for producing Y values in specified regions, such as extreme high/low or any behavioural classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions and comments\n",
    "### SALib Documentation is very useful for understanding this part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.15658296 0.8483589 ]\n",
      "[0.15499711 0.84653102]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import SALib as slb\n",
    "from SALib.sample import saltelli\n",
    "from SALib.analyze import sobol\n",
    "problem = {\n",
    "    'num_vars': 2,\n",
    "    'names': ['x1', 'x2' ],\n",
    "    'bounds': [[-3.14159265359, 3.14159265359],\n",
    "               [-3.14159265359, 3.14159265359]]\n",
    "    }\n",
    "X = saltelli.sample(problem,1000)###sampling \n",
    "F1 = 0.3*X[:, 0] + 0.7*X[:, 1]###the output \n",
    "S1=sobol.analyze(problem,F1)###analyze part\n",
    "print (S1['S1'])\n",
    "print (S1['ST'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in this funtion we get the result that X2 exhibits higher first-order sensitivities than X1,and the total-order indices are almost the same as the first order indices so maybe it has no interaction with other parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.496712   0.53376079]\n",
      "[0.48834383 0.52334649]\n"
     ]
    }
   ],
   "source": [
    "F2 = np.sin(np.pi*X[:, 0] )+ np.cos(9*np.pi*X[:, 1])\n",
    "S2=sobol.analyze(problem,F2)\n",
    "print (S2['S1'])\n",
    "print (S2['ST'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in this funtion we get the result that X2 exhibits a bit higher first-order sensitivities than X1,and the total-order indices are a bit lower than the first order indices.X2 also exhibit a bit higher first-order sensitivities than X1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.14927928 0.68727345]\n",
      "[0.33865575 0.8692273 ]\n",
      "x1-x2: 0.1910737437900184\n"
     ]
    }
   ],
   "source": [
    "F3 = np.exp(0.3*X[:, 0] + 0.7*X[:, 1])\n",
    "S3=sobol.analyze(problem,F3)\n",
    "print (S3['S1'])\n",
    "print (S3['ST'])\n",
    "print (\"x1-x2:\", S3['S2'][0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in this funtion we get the result that X2 exhibit a much higher first-order sensitivities than X1,and the total-order indices are substantially larger than the first order indices,which means there is likely higher-order interactions occurring.but X2 still has a higher influence than X1 in total indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.02744045 0.9750393 ]\n",
      "[0.0278985  0.97587372]\n"
     ]
    }
   ],
   "source": [
    "F4 = (1./(1+np.exp(-100*X[:, 0]))) + (-1*X[:, 1]**2)\n",
    "S4=sobol.analyze(problem,F4)\n",
    "print (S4['S1'])\n",
    "print (S4['ST'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in this funtion we get the result that X2 exhibit a much higher first-order sensitivities than X1,and the total-order indices are almost the same as the first order indices,which means there is no  higher-order interactions occurring.but X2 still has a higher influence than X1 in total indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.10351966 0.89850748]\n",
      "[0.10567835 0.90014666]\n"
     ]
    }
   ],
   "source": [
    "F5=(1./(1+np.exp(-100*X[:, 0]))) + (-0.8*X[:, 1]**1)\n",
    "S5=sobol.analyze(problem,F5)\n",
    "print (S5['S1'])\n",
    "print (S5['ST'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in this funtion we get the result that X2 exhibit a much higher first-order sensitivities than X1,and the total-order indices are almost the same as the first order indices,which means there is no  higher-order interactions occurring.but X2 still has a higher influence than X1 in total indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.06136138 0.00019603]\n",
      "[1.21333211 0.96818528]\n",
      "x1-x2: 1.2077086476206622\n"
     ]
    }
   ],
   "source": [
    "F6=(np.exp(-X[:, 0]**2 /0.001))*(10*X[:, 1])\n",
    "S6=sobol.analyze(problem,F6)\n",
    "print (S6['S1'])\n",
    "print (S6['ST'])\n",
    "print (\"x1-x2:\", S6['S2'][0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can see in the result that X2 seems to have no influence on the output and X1 only has a little influence on the output through the \n",
    "first-order sensitivities,also through the total-order indices we know that there is likely higher-order interactions occurring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.25320441e-04 1.00331113e+00]\n",
      "[0.00194826 1.0039382 ]\n"
     ]
    }
   ],
   "source": [
    "F7=(-10*np.exp(-X[:, 0]**2 /0.001))+(10*X[:, 1])\n",
    "S7=sobol.analyze(problem,F7)\n",
    "print (S7['S1'])\n",
    "print (S7['ST'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in the first-order indices,we know that X1 almost has no influence compared to X2,and the total-order indices are almost the same as the first order indices,which means there is no  higher-order interactions occurring.X2 still has a higher influence than X1 in total indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.00850205 -0.98076264]\n",
      "[1.76138726 1.18663099]\n",
      "x1-x2: 1.2077086476206622\n"
     ]
    }
   ],
   "source": [
    "F8=(X[:, 0]/X[:, 1])\n",
    "S8=sobol.analyze(problem,F8)\n",
    "print (S8['S1'])\n",
    "print (S8['ST'])\n",
    "print (\"x1-x2:\", S6['S2'][0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in the first-order indices,X2 appears to have no first-order effect,and X1 has only a little influence.but the total-order indices are much higher than the first-order indices,then there is likely higher-order interactions occurring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 5.84950923e-04 -7.95071346e-01]\n",
      "[1.50508576 1.47290727]\n"
     ]
    }
   ],
   "source": [
    "F9=(X[:, 0]/(X[:, 1]+1.1))\n",
    "S9=sobol.analyze(problem,F9)\n",
    "print (S9['S1'])\n",
    "print (S9['ST'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in the first-order indices,X2 appears to have no first-order effect,and X1 has huge influence,in the total-order indices,X1 and X2 seem to have equal effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Morris sensitivity analysis\n",
    "the Morris method for global sensitivity analysis is a so-called one-step-at-a-time method (OAT), meaning that in each run only one input parameter is given a new value. It facilitates a global sensitivity analysis by making a number r of local changes at different points x(1 → r) of the possible range of input values.[2]\n",
    "\n",
    "Steps:\n",
    "+ sample a set of values with defined range,which is used for inputs and then calculate the output.\n",
    "+ only change 1 variable can work out the output.\n",
    "+ repeat step two until all variables are changed.this process is usually repeated between 5 and 15 times.Such number is large enough compared to more demanding methods for sensitivity analysis.\n",
    "\n",
    "A sensitivity analysis method widely used to screen factors in models of large dimensionality is the design proposed by Morris.[3] The Morris method deals efficiently with models containing hundreds of input factors without relying on strict assumptions about the model, such as for instance additivity or monotonicity of the model input-output relationship. The Morris method is simple to understand and implement, and its results are easily interpreted. Furthermore, it is economic in the sense that it requires a number of model evaluations that is linear in the number of model factors. The method can be regarded as global as the final measure is obtained by averaging a number of local measures (the elementary effects), computed at different points of the input space.[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter                         Mu_Star         Mu    Mu_Star_Conf      Sigma\n",
      "x1                                  7.792      7.792           0.404      6.251\n",
      "x2                                  7.875      0.158           0.000      7.877\n",
      "x3                                  6.211      0.137           0.382      8.814\n"
     ]
    }
   ],
   "source": [
    "#morris\n",
    "import numpy as np\n",
    "import SALib as slb\n",
    "from SALib.analyze import morris\n",
    "from SALib.sample import morris\n",
    "from SALib.test_functions import Ishigami\n",
    "problem = {\n",
    "    'num_vars': 3,\n",
    "    'names': ['x1', 'x2','x3'],\n",
    "    'bounds': [[-3.14159265359, 3.14159265359],\n",
    "               [-3.14159265359, 3.14159265359],\n",
    "               [-3.14159265359, 3.14159265359]]\n",
    "    }\n",
    "X = slb.sample.morris.sample(problem, 1000, num_levels=4, grid_jump=2)#sampling\n",
    "Y=Ishigami.evaluate(X)###the output \n",
    "Si=slb.analyze.morris.analyze(problem, X, Y, conf_level=0.95,print_to_console=True, num_levels=4, grid_jump=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to understand the result,we should first understand the Ishigami funtion,this funtion is defined as:\n",
    "$$Y=g(X_1,X_2,X_3)=sin(X_1)+asin^2(X_2)+bX_3^4sin(X_1)$$\n",
    "this model exhibits strong nonlinearity and nonmonotonicity. It also has a peculiar dependence on $X_3$\n",
    "in this model,$X_1,X_2,X_3$are independent random variables uniform in $[-\\pi,\\pi]$\n",
    "the value of a,b is a=7,b=0.1\n",
    "\n",
    "The mean and variance of the three inputs are represented by Mu and Sigma.Mu_star is the mean of the absolute values of the elementary effects,which is the best approximation of total sensitivity.However,it is not a direct interpretation as an attribution of variance,we should consider the Mu_star as a rank of the most sensitivity parameters,and in the above outcome,we can learn that X_1 is the most significant part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sensitivity analysis in metrology \n",
    "+ Derivative-based Global Sensitivity Measure\n",
    "\n",
    "The method is based on averaging local derivatives using Monte Carlo sampling methods. This method is much more accurate than the Morris method as the elementary effects are evaluated as local derivatives with only a small increments compared to the variable uncertainty ranges. It becomes especially efficient if automatic calculation of derivatives is used.[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter vi vi_std dgsm dgsm_conf\n",
      "x1 7.622378 16.198123 2.207554 1.001117\n",
      "x2 24.487757 17.338556 7.092019 1.026800\n",
      "x3 11.181258 24.062127 3.238259 1.531967\n"
     ]
    }
   ],
   "source": [
    "#Derivative-based Global Sensitivity Measure\n",
    "import numpy as np\n",
    "import SALib as slb\n",
    "from SALib.analyze import dgsm\n",
    "from SALib.sample import finite_diff\n",
    "from SALib.test_functions import Ishigami\n",
    "problem = {\n",
    "    'num_vars': 3,\n",
    "    'names': ['x1', 'x2','x3'],\n",
    "    'bounds': [[-3.14159265359, 3.14159265359],\n",
    "               [-3.14159265359, 3.14159265359],\n",
    "               [-3.14159265359, 3.14159265359]]\n",
    "    }\n",
    "X= finite_diff.sample(problem, 1000,delta=0.001)## Generate samples\n",
    "Y=Ishigami.evaluate(X)###the output \n",
    "Si=dgsm.analyze(problem, X, Y, num_resamples=1000, conf_level=0.95, print_to_console=True)###analyze part\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$V_i=E[(\\frac{\\partial Y}{\\partial x_i})^2]$,the index is motivated by the fact that a high value of the derivative of the model output with respect to any input variable leads to much variation of model output,so a higher value means a higher impact on the output of model,we can see that X2 is the highest among the three input,so X2 has a larger influence than th others,vi_std is the standard deviation of Vi,dgsm is the upper-bound to the total.[6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ moment-independent sensitivity analysis\n",
    "\n",
    "The moment-independent SA focuses on finding those inputs that, if fixed at their distribution ranges, will lead to the greatest shift in the probability density function (PDF) of model output on average. It is also global, quantitative and model free, and additionally, it is moment-independent, thus attracts more and more attentions of practitioners recently. [7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter delta delta_conf S1 S1_conf\n",
      "x1 0.177752 0.026942 0.275467 0.037069\n",
      "x2 0.225945 0.023294 0.277506 0.045769\n",
      "x3 0.129075 0.025875 0.009239 0.015652\n"
     ]
    }
   ],
   "source": [
    "#Delta Moment-Independent Measure\n",
    "import numpy as np\n",
    "import SALib as slb\n",
    "from SALib.sample import latin\n",
    "from SALib.analyze import delta\n",
    "from SALib.test_functions import Ishigami\n",
    "problem = {\n",
    "    'num_vars': 3,\n",
    "    'names': ['x1', 'x2','x3'],\n",
    "    'bounds': [[-3.14159265359, 3.14159265359],\n",
    "               [-3.14159265359, 3.14159265359],\n",
    "               [-3.14159265359, 3.14159265359]]\n",
    "    }\n",
    "X= slb.sample.latin.sample(problem, 1000)## Generate samples\n",
    "Y=Ishigami.evaluate(X)###the output \n",
    "Si=slb.analyze.delta.analyze(problem,X, Y, conf_level=0.95, print_to_console=True)###analyze part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "delta is defined as $\\delta_i=\\frac{1}{2}E_{x_i}[s(Xi)]=\\frac{1}{2}\\int \\mid f_y(y)-f_{y\\mid x_i}(y)\\mid dy$,$\\delta_i$ represents the normalized expected shift in the distribution of Y provoked by Xi,we can see that X2 is the most influential parameter, S1 is the first-order indices we can also get the information that x2 is the most influential parameter[8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1] https://en.wikipedia.org/wiki/Variance-based_sensitivity_analysis [Accessed 8 Mar, 2018]\n",
    "\n",
    "[2] https://en.wikipedia.org/wiki/Morris_method [Accessed 8 Mar, 2018]\n",
    "\n",
    "[3] Morris, M.D. (1991). \"Factorial Sampling Plans for Preliminary Computational Experiments\" . Technometrics. 33: 161–174. doi:10.2307/1269043.\n",
    "\n",
    "[4] Campolongo, F.; Cariboni, J.; Saltelli, A. (2003). \"Sensitivity analysis: the Morris method versus the variance based measures\" . Submitted to Technometrics.\n",
    "\n",
    "[5]I.M. Sobol’a, S. Kucherenkob*.\"Derivative based global sensitivity measures \".Procedia Social and Behavioral Sciences 2 (2010) 7745–7746\n",
    "\n",
    "[6]Matthias De Lozzo, Amandine Marrel. Estimation of the derivative-based global sensitivity measures using a Gaussian process metamodel. 2015. <hal-01164215v2>\n",
    "\n",
    "[7]Pengfei Wei, Zhenzhou Lu n, Xiukai Yuan.\"Monte Carlo simulation for moment-independent sensitivity analysis\".Reliability Engineering and System Safety 110 (2013) 60–67\n",
    "\n",
    "[8]E. Borgonovo.A new uncertainty importance measure.Institute for Quantitative Methods, Bocconi University, 20135 Milano, Viale Isonzo 25, Italy"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
