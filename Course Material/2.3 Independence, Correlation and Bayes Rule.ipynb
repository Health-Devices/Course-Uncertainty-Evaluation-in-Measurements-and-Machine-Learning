{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture scribing template\n",
    "## Objectives\n",
    "<ul>\n",
    "<li>Independence</li>\n",
    "<li>Co-variance properties</li>\n",
    "<li>Correlation Coefficient and its interpretation</li>\n",
    "<li>Some basic information about Multivariate Gaussian distribution equation</li>\n",
    "<li>Bayes's rule and Bayesian inference framework</li>\n",
    "<li>Communication example</li>\n",
    "<li>Use of these in measurement theory</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Independence\n",
    "### Independence in Probability\n",
    "#### Independence\n",
    "\n",
    "Conditional probability $P(A \\mid B)$ is introduced to provide the some information that event B tells about event A. What if the occurance of B provides no information and had no effect on occurance of A.\n",
    "i.e;\n",
    "$$P(A \\mid B) = P(A)$$\n",
    "\n",
    "The above equation is when A is independent of B.By the definition $P(A \\mid B) = \\frac{P(A \\cap B)}{P(B)}$, it is equal to\n",
    "\n",
    "$$P(A \\cap B) = P(A)P(B)$$\n",
    "\n",
    "If A is independent of B, then B is independent of A, and we can unambiguously say that A and B are independent events.\n",
    "\n",
    "For example, if the occurrence of two events had no effect on each other, such events are said to be independent. \n",
    "As it is difficult to predict that 2 events are independent in terms of sample space, we cann think that 2 events are disjoint then they are independent to each other, as disjoint events have no intersecion part i.e; Two disjoint events A and B with $P(A) > 0$ and $P(B) > 0$ are never independent, since their intersection $ A \\cap B$ is empty and has probability 0....[1] \n",
    "\n",
    "For example, an event A and its complement $A^c$ are not independent [unless $P(A) = 0$ or $P(A) = 1$], As we know there is a relation between occurance and non-occurance of an event.\n",
    "\n",
    "##### Example :\n",
    "A coin is tossed and a single 6-sided die is rolled. Find the probability of landing on the tail side of the coin and rolling a 4 on the die.....[8]\n",
    "\n",
    "Probabilities: \n",
    "$$P(tail)\t = \t\\frac{1}{2}$$\n",
    "  \n",
    "$$P(4)       =  \\frac{1}{6}$$\n",
    "  \n",
    "$$P(tail and 4)= P(tail)·P(4)$$\n",
    "\n",
    "$$=\\frac{1}{2}·\\frac{1}{6}$$\n",
    "\n",
    "$$=\\frac{1}{12}$$\n",
    "\n",
    "#### Conditional Independence \n",
    "Conditional probability is the probability of occurance of an event when given other event is true or had already occured. In particular, given an event C. the events A and B are called conditionally independent....[9]\n",
    "\n",
    "if\n",
    "$$P(A \\cap B \\mid C) =p(A \\mid C)P(B \\mid C)$$\n",
    "\n",
    "In another way, we use the definition of the conditional probability and the multiplication rule, to write \n",
    "\n",
    "$$P(A \\cap B \\mid C) = \\frac{p(A \\cap B \\cap C)}{P(C)} $$\n",
    "\n",
    "$$=\\frac{P(C)P(B \\mid C)P(A \\mid B \\cap C)}{ P(C)}$$\n",
    "\n",
    "$$=P(B \\mid C) P(A \\mid B \\cap C)$$\n",
    "\n",
    "By comparing the above two equations we get\n",
    "\n",
    "$$P(A \\mid B \\cap C) = p(A \\mid C)$$ \n",
    "\n",
    "This equation tells us that If C is known to have occured,then the occurence of B had no effect on the occurence of A.\n",
    "\n",
    "##### Example : \n",
    "From a total of 50 people surveyed in a study, 35 drinks alcohol in which there are 20 males. What is the probability the if the person surveyed drinks alcohol then he is a male?\n",
    "\n",
    "Solution: \n",
    "\n",
    "Probability of the person being male and drinks alcohol, \n",
    "$$P(A \\cap B) = \\frac{20}{50}$$\n",
    "\n",
    "Probability of person drinks alcohol, \n",
    "$$P(A) = \\frac{35}{50}$$\n",
    "\n",
    "Probability of a person being male if he drinks alcohol, \n",
    "$$P(B \\mid A)= \\frac{P(A \\cap B)}{P(A)}= \\frac{20}{35} = \\frac{1}{7}$$\n",
    "\n",
    "Hence, the compound probability that if the person surveyed drinks alcohol then he will be a male = $\\frac{1}{7}$\n",
    "\n",
    "#### Independence in Probability Summary\n",
    "• Two events A and B are said to be independent if\n",
    "\n",
    "$$P(A \\cap B) = P(A)P(B)$$\n",
    "\n",
    "and If A and B are independent events then\n",
    "$$P(A \\mid B) = P(A)$$\n",
    "\n",
    "• If A and B are independent, so are A and $B^c$\n",
    "\n",
    "• Two events A and B are said to be conditionally independent, given another event C with $P(C) > 0$, if \n",
    "\n",
    "$$p(A \\cap B \\mid C) = P(A \\mid C)P(B \\mid C)$$ \n",
    "\n",
    "If in addition, $P(B \\cap C) > 0$, conditional independence is equivalent to the condition\n",
    "\n",
    "$$P(A \\mid B \\cap C) = P(A \\mid C)$$\n",
    "\n",
    "• Independence and conditional independence are different. \n",
    "\n",
    "\n",
    "\n",
    "### Independence with respect to random variables\n",
    "\n",
    "#### Independence of a Random Variable from an Event \n",
    "The independence of a random variable from an event is similar to the independence of two events.We can say that the random variable X is independent of the event A if\n",
    "\n",
    "$$P(X=x \\quad and \\quad A)= P(X=x)P(A)$$\n",
    "\n",
    "$$=p_X(x)P(A) \\quad \\textrm {for all x} \\quad $$\n",
    "\n",
    "From the definition of the conditional Probability mass function, we have \n",
    "\n",
    "$$P(X=x and A)= p_{X \\mid A}(x)P(A)$$\n",
    "\n",
    "so if $P(A) > 0$ , independence condition is same as\n",
    "\n",
    "$$p_{X \\mid A}(x)=p_X(x) \\quad \\textrm {for all x} \\quad $$\n",
    "\n",
    "#### Independence of Random Variables \n",
    "\n",
    "If occurance of events in random variabe X had no effect on the occurance of events in random variable Y then we can say X and Y variables are independent of each other.The two random variables X and Y are independent if\n",
    "\n",
    "$$p_{X,Y}(x, y) = p_X(x) p_Y (Y) \\quad \\textrm{for all x, y} \\quad $$ \n",
    "\n",
    "This is the same that ${X = x}$ and ${Y = y}$ be independent for every x and y. and this formula $p_{X,Y}(x, y) = p_{X \\mid Y}(x \\mid y)p_y(y)$ shows that independence is equivalent to the condition \n",
    "\n",
    "$$p_{X \\mid Y}(x  y) = p_X(x) \\quad \\textrm{for all y with}  \\quad p_y(y) > 0 \\quad \\textrm{and all x} \\quad $$\n",
    "\n",
    "Independence means that the value of variable Y provides no information on the value of variable X. \n",
    "\n",
    "$$ \\quad \\textrm {if} P(X = x, Y = y \\mid A) = P(X = x \\mid A)P(Y = y \\mid A)  \\quad \\textrm{for all x and y} \\quad $$\n",
    "\n",
    "or \n",
    "\n",
    "$$p_{X,Y \\mid A}(x,y)=p_{X \\mid A}(x)p_{Y \\mid A}(y)  \\quad \\textrm{for all x and y} \\quad $$\n",
    "\n",
    "If X and Y are independent random variables, then \n",
    "\n",
    "$$E[XY] = E[X] E[Y]$$ \n",
    "\n",
    "as shown by the following calculation:\n",
    "\n",
    "$$E[XY] = \\sum_{x} \\sum_{y} xy p_{X,Y}(x,y) $$\n",
    "\n",
    "$$=\\sum_{x} \\sum_{y}xyp_X(x)p_Y(y)$$\n",
    "\n",
    "$$=\\sum_{x}xp_X(x) \\sum_{y}yp_Y(y)$$ \n",
    "\n",
    "$$= E[X] E[Y]$$\n",
    "\n",
    "We can also say that if X and Y are independent, then\n",
    "\n",
    "$$E[g(x).h(y)]=E[g(x)].E[h(y)]$$\n",
    "\n",
    "for any functions g and h.Infact this follows immediately once we realize that if X and Y are independent then the same is true for $g(X)$ and $h(Y)$.\n",
    "\n",
    "Now lets find the variance of the sum X+Y where X and Y are two random variables.Since the variance of a random variable is unchanged when the random variable is shifted by a constant,it is convenient to work with the zero-mean random variables $\\bar X= X - E[X]$ and $\\bar Y= Y- E[Y]$\n",
    "\n",
    "we have \n",
    "\n",
    "$$ var(X+Y)= var(\\bar X + \\bar Y)$$\n",
    "\n",
    "$$=E[(\\bar X + \\bar Y)^{2}]$$\n",
    "\n",
    "$$=E[\\bar X^2 + \\bar Y^2 + 2 \\bar X \\bar Y]$$\n",
    "\n",
    "$$=E[\\bar X^2]+ E[\\bar Y^2] + 2 E[\\bar X \\bar Y]$$\n",
    "\n",
    "$$=E[\\bar X^2]+ E[\\bar Y^2]$$\n",
    "\n",
    "$$=var(\\bar X)+var(\\bar Y)$$\n",
    "\n",
    "$$=var(X)+var(Y)$$\n",
    "\n",
    "In the above we took $E[\\bar X \\bar Y] = 0$ because the random variables $\\bar X = X - E[X]$ and $Y = Y - E[Y]$ are independent (because they are functions of the independent random variables X and Y) and have zero mean.\n",
    "$$E[X Y] = E[X] E[Y] = 0$$\n",
    "\n",
    "So we can say that the variance of the sum of two independent random variables is equal to the sum of their variances and the mean of the sum of two random variables is always equal to the sum of their means. even if they are not independent. \n",
    "\n",
    "#### Summary of Facts About Independent Random Variables\n",
    "\n",
    "Let us consider X and Y are two random variables and A be the event associated with them, P(A)>0.\n",
    "\n",
    "• X is independent of the event A if \n",
    "$$ p_{X \\mid A}(x) = p_X(x), \\quad \\textrm {for all x} \\quad $$ \n",
    "\n",
    "if X and Y are two independent random variables then\n",
    "\n",
    "$$ p_{X,Y}(x, y) = p_X(x)p_Y(y), \\quad \\textrm {for all x, y} \\quad.$$ \n",
    "\n",
    "•Two variables are said to be independent if:\n",
    "$$p(x \\mid y)=p(x)p(y)$$\n",
    "$$p(x \\mid y)=p(x)p(y)$$\n",
    "\n",
    "•if X and Y are independent, the conditional probability of X given Y is equal to the probability of X :\n",
    "$$p(x \\mid y)=p(x)$$\n",
    "$$p(x \\mid y)=p(x)$$\n",
    "\n",
    "•Two variables X and Y and said to be conditionally independent of Z if the following are true:\n",
    "$$p(x,y \\mid z)=p(x \\mid z)p(y \\mid z)$$\n",
    "$$p(x,y \\mid z)=p(x \\mid z)p(y \\mid z)$$\n",
    "\n",
    "•if X,Y,Z are three random variables and X,Y are conditionally independent of Z then we can also say\n",
    "$$p(x \\mid z,y)=p(x \\mid z)$$\n",
    "$$p(x \\mid z,y)=p(x \\mid z)$$\n",
    "\n",
    "• If X and Y are independent random variables, then \n",
    "\n",
    "$$E[XY] = E[X] E[Y]$$\n",
    "\n",
    "Furthermore, for any functions 9 and h, the random variables g(X) and h(Y) are independent, and we have\n",
    "\n",
    "$$E[g(X)h(Y)] = E[g(X)] E[h(Y)]$$\n",
    "\n",
    "• If X and Y are independent, then\n",
    "\n",
    "$$var(X + Y) = var(X) + var(Y)$$\n",
    "\n",
    "#### Independence of Several Random Variables \n",
    "If X,Y,Z are three random variables and each of them is independent of one another then we can write \n",
    "\n",
    "$$p_{X.Y.Z}(x. y. z) = p_x(x)p_y(y)p_z(z) \\quad \\textrm {for all x , y , z} \\quad$$  \n",
    "\n",
    "If X, Y and Z are independent random variables hen any three random variables of the form $f(X)$. $g(Y)$. and $h(Z)$, are also independent,any two random variables of the form $g(X, Y)$ and $h(Z)$ are independent.\n",
    "\n",
    "But two random variables of the form $g(X, Y)$ and $h(Y, Z)$ are usually not independent because they are both affected by Y.\n",
    "\n",
    "\n",
    "#### Variance of the Sum of Independent Random Variables \n",
    "\n",
    "Calculation of Sums of independent random variables are important because they arise in various statistical applications where we \"average\" a number of independent measurements, in the process of minimizing the effects of measurement errors.\n",
    "\n",
    "If X_1, X_2,...., X n are independent random variables, then\n",
    "\n",
    "$$var(X_l + X_2 + ... + X_n) = var(X_1) + var(X_2) + ... + var(X_n)$$\n",
    "\n",
    "This can be verified by using $var(X + Y) = var(X) +var(Y)$ for two independent random variables X and Y. \n",
    "\n",
    "### Independence with respect to Continous random Variables\n",
    "\n",
    "Let X,Y are two continous random variables which are independent of each other.\n",
    "\n",
    "$$f_{X,Y}(x,y)=f_X(x)f_Y(y)$ \\quad \\textrm{for all x,y} $$\n",
    "\n",
    "$$f_{X,Y}(x,y)=f_{X \\mid Y)}(x \\mid y)f_Y(y)$$\n",
    "\n",
    "$$f_{X \\mid Y)}(x \\mid y)=f_X(x) \\quad \\textrm{for all y with }  \\quad fY(y)>0 \\quad \\textrm{ and all }x$$\n",
    "\n",
    "here $f_X$ , $f_Y$ are probability density functions of X,Y\n",
    "$f_{X,Y}$ is the joint density function of X,Y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Covariance Properties\n",
    "\n",
    "#### COVARIANCE\n",
    "The measure of joint variability between two random variables is called covariance. This is with respect to probability theory and statistics.\n",
    "These variables shows similar behaviour when the greater values in one variable correspond to the greater values of another variable,this type of behaviour tells the covariance is positive.In the same way the variables shows opposite behaviour if the greater values in one variable correspond to the lesser values of another variable,this shows that the covariance is negative.Covariance tells linear relationship between variables.It's magnitude depends on the magnitude of the other variables because as the covariance is not normalized it is difficult to understand the magnitude of covariance.....[1]\n",
    "\n",
    "The covariance is denoted by $cov(X,Y)$ .here $X$ and $Y$ are random variables.\n",
    "Covariance is defined as  \n",
    "\n",
    "$$ cov(X,Y)=E[(X-E[X])(Y-E[Y]) $$\n",
    "\n",
    "If cov(X,Y)=0 then the two random variables $X$ and $Y$ are said to be uncorrelated.Generally Covariance is used to show the relationship between random variables.In terms of $X$ and $Y$,if the Values of $X-E[X]$ and $Y-E[Y]$ (which we get in a single experiment) have the same sign then we can say the covariance is positive and in the same way if the values of $X-E[X]$ and $Y-E[Y]$ are of opposite sign then we can say that the covaiance is negative........[1]\n",
    "\n",
    "Below Figure....(Copied from [3])\n",
    "<img src=\"Positive_and_Negative_Covariance.PNG\">\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "In the above figure (a)$Cov(X,Y)$=positive (b) $Cov(X,Y)$=negative\n",
    "\n",
    "As we know \n",
    "\n",
    "$$cov(X,Y)=E[(X-E[X])(Y-E[Y])$$\n",
    "\n",
    "$$=E[XY-XE[Y]-E[X]Y+E[X]E[Y]]$$\n",
    "\n",
    "$$=E[XY]-E[X]E[Y]-E[X]E[Y]+E[X]E[Y]$$\n",
    "\n",
    "$$=E[XY]-E[X]E[Y]$$\n",
    "\n",
    "This is another formula for covariance\n",
    "\n",
    "$$cov(X,Y)=E[XY]-E[X]E[Y]$$\n",
    "\n",
    "### PROPERTIES OF COVARIANCE\n",
    "\n",
    "1)If the two variables are identical then the covariance of the two variables is equal to the variance of the single variable\n",
    "\n",
    "$$cov(X,X)= var(X)= \\sigma_X^2$$\n",
    "\n",
    "2)Let us Consider $X$,$Y$,$W$,$Z$ as real valued random variables and a,b,c,d as constants then some of the definitions of covariance will be\n",
    "\n",
    "$$cov(X,a)=0$$\n",
    "\n",
    "$$cov(X,X)=var(X)$$\n",
    "\n",
    "$$cov(X,Y)=cov(Y,X)$$\n",
    "\n",
    "$$cov(aX,bY)=abcov(X,Y)$$\n",
    "\n",
    "$$cov(X+a,Y+b)=cov(X,Y)$$\n",
    "\n",
    "$$cov(aX+bY,cW+dZ)=ac cov(X,W)+ ad cov(X,Z) + bc cov(Y,W) + bd cov(Y,Z)$$\n",
    "\n",
    "$$cov(X,aY+b)=a.cov(X,Y)$$\n",
    "\n",
    "$$cov(X,Y+Z)=cov(X.Y)+cov(X,Z)$$\n",
    "\n",
    "$$cov(aX+b,Y)=acov(X,Y)$$\n",
    "\n",
    "\n",
    "3)The covariance of two independent random variables is zero.this can be derived by using the formula of mean.\n",
    "if two variables $X$,$Y$ are independent then $E[XY]=E[X].E[Y]$\n",
    "\n",
    "$$cov(X,Y)=0$$\n",
    "\n",
    "4) If $X_1$,$X_2$,$X_3$,$X_4$,........$X_{n-1}$,$X_n$ are n random variables then the variance of sum of those variables is given by\n",
    "\n",
    "$$var(X_1+X_2+......+X_n)= \\sum_{i=1}^{n} var(X_i)+ \\sum_{(i,j):i \\neq j} cov(X_i,X_j)$$\n",
    "\n",
    "By using this the variance of a sum of two random varibles $X_1$,$X_2$ is\n",
    "\n",
    "$$var(X_1+X_2)=var(X_1)+var(X_2)+2cov(X_1,X_2)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation Coefficient and it's intrepretation\n",
    "\n",
    "### Correlation\n",
    "\n",
    "The numerical measurement of the statistical relationship between two variables(Correlation) is known as correlation coefficient.\n",
    "\n",
    "There are many types of correlation but we generally use Pearson product-moment Correlation. The value of correlation coefficient varies between -1 to 1 .\n",
    "\n",
    "\n",
    "If the correlation coefficient of two variables X and Y are equal to 1 then they are said to be in perfect correlation.In the same way if the value of correlation coefficient is 0 then they is no correlation between the variables.If the Correlation Coefficient is -1 then the two variables are said to be in perfectly anticorrelation.....[1]\n",
    "\n",
    "The correlation coefficient of two random variables $X$,$Y$ which are having non zero variances is defined as\n",
    "\n",
    "$$\\rho (X,Y)= \\frac{cov(X,Y)}{\\sqrt{var(X)var(Y)}}$$\n",
    "\n",
    "$$\\rho (X,Y)= E \\left[\\frac{(X-E[Y])}{\\sigma_X} , \\frac{(Y-E[Y])}{\\sigma_Y}  \\right]$$\n",
    "\n",
    "$$\\rho (X,Y)= \\frac{cov(X,Y)}{\\sigma_X . \\sigma_Y}$$\n",
    "\n",
    "$$-1 \\leq \\rho \\leq 1$$\n",
    "\n",
    "if the random variables $X$ , $Y$ are independent then their correlation coefficient is $\\rho$=0\n",
    "\n",
    "Correlation coefficient is defined as the measure of degree of \"association\" between the  random variables $X$ and $Y$.\n",
    "\n",
    "If the correlation correlation coefficient of two random variables $X$,$Y$ is 1 i.e $\\rho$=1 then they are said to be linearly dependent.\n",
    "\n",
    "### intrepreting the correlation coefficient\n",
    "\n",
    "As we have seen that the correlation is the degree of 'association' between the random variables.here association does not mean causation or influence\n",
    "\n",
    "In general survey says that the people who are good at mathematics are also good at music.But they are not depend on each other like to excel in maths one need not to learn music.\n",
    "\n",
    "Assume $X$ is the random variable which shows the mathematical ability, $Y$ is the variable which shows the musical ability, $Z$ is the variable which tells about the hardwork to excel in a particular field, $V$ is variable which tells the mathematical talent of the person, $W$ is the random variable which tells the talent of the person in musical ability.We know that $Z,V,W$ are independent of each other and all of them follows standard normal distribution i.e mean=0, variance=1.Correlation generally shows underlying,common or hidden factor....[1]\n",
    "\n",
    "As the mathematical ability of the person can be exceled if he had done both hardwork and mathematical talent.\n",
    "\n",
    "$$X=Z+V$$\n",
    "\n",
    "Musical ability can be achieved by hardwork and musical talent\n",
    "\n",
    "$$Y=Z+W$$\n",
    "\n",
    "Now let us calculate the correlation coefficient of $X$,$Y$\n",
    "\n",
    "$$\\rho (X,Y)= \\frac{cov(X,Y)}{\\sigma_X . \\sigma_Y}$$\n",
    "\n",
    "$$cov(X,Y)=E[(Z+V)(Z+W)]$$\n",
    "\n",
    "$$=E[Z^2]+E[ZV]+E[ZW]+E[VW]$$\n",
    "\n",
    "$$=1+0+0+0 \\quad \\textrm { As $Z$,$V$,$W$ have zero mean and independent events} \\quad $$\n",
    "\n",
    "$$cov(X,Y)=1$$\n",
    "\n",
    "$$\\sigma_X=\\sqrt {var(X)} , \\sigma_Y=\\sqrt {var(Y)}$$\n",
    "\n",
    "As $var(X)$,$var(Y)$ are 1\n",
    "\n",
    "$$\\sigma_X=1 , \\sigma_Y=1 $$\n",
    "\n",
    "$$ \\textrm {so} \\quad \\rho (X,Y)= 1$$\n",
    "\n",
    "This means X,Y are in Perfect Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes' Rule and Interference model\n",
    "\n",
    "### Bayes' Rule with respect to probability\n",
    "\n",
    "Let us Consider x,y are the two events.p(x) is the probability of occurence of the event x and p(y) is the probability of occurence of the event y.Then \n",
    "\n",
    "$$p(x\\mid y) = \\frac{p(y\\mid x) \\, p(x) }{p(y)} $$\n",
    "\n",
    "$p(y \\mid x)$ is the probability of occurence of the event y when x is true or given. \n",
    "\n",
    "$p(x \\mid y)$ is the probability of occurence of the event x when y is true or given\n",
    "\n",
    "we know that joint probability of x,y is the probability of occurence of both events at a time.\n",
    "\n",
    "$p(x,y)$ is the joint probability and it can be written as\n",
    "\n",
    "$$p(x,y)=p(y \\mid x)p(x)=p(x \\mid y)p(y)$$\n",
    "\n",
    "#### Example \n",
    "\n",
    "Consider a medical test for a disease. The test has a probability of 0.95 of correctly or positively detecting an infected person (this is the sensitivity), while it has a probability of 0.90 of correcctly identifying a healthy person (this is called specificity). In the population, only 3% of the people have the disease.  \n",
    "\n",
    "(a) What is the probability that a person testing positive is actually infected? \n",
    "\n",
    "(b) What is the probability that a person testing negetive is actually infected? ........[10]\n",
    "\n",
    "##### solution\n",
    "$$ p(y\\mid x) = \\frac{p(x\\mid y) \\, p(y) }{p(x)} $$\n",
    "<p> P(y) is the probability of event y </p>\n",
    "<p> P(x) is the probability of event x </p>\n",
    "<p> P(x|y) is the probability of observing event x, if y is true </p>\n",
    "<p> P(y|x) is the probability of observing event x=y, if x is true </p>\n",
    "<p>The information that is available is as follows:</p>\n",
    "<p>P(Test Identifying infection(Test positive)| Infected)=0.95, i.e., the probability of , given the presence of infection is 95% (the sensitivity of the test), and </p>\n",
    "<p>P(Test Identifying Healthiness(Test negative) | Healthy)=0.90, i.e., the probability of detecting the healthy person is 90% or 0.09. We can now substitute the values into the above equation to compute the desired probability </p>0.003, i.e., prevalence = 0.003</p>\n",
    "<p>Here the **Test identifying infection** means **testing positive** and **Test identifying healthiness** means **testing negative.**</p>\n",
    "\n",
    "<p> Based on the available information, we could piece this together using a hypothetical population of 10,000 people. Given the available information this test would produce the results summarized in the table below.</p> \n",
    "\n",
    "|                                                    | Infected           | Healthy  |   Total    |\n",
    "| ---------------------------------------------------|:------------------:| --------:|-----------:|\n",
    "| **Test Identifying Infection**(Test positive)      | 285                |  970     |  1255      |\n",
    "| **Test Identifying Healthiness**(Test negative)    | 15                 |  8730    |  8745      | \n",
    "|  Total                                             | 300                |  9700    |  10000     |\n",
    "\n",
    "<p>Given The percentage of the people have the Infection is 3% or 3 per 10000. Therefore, in 10,000 screened people, there will be 300 who are infected.</p>\n",
    "<p> As there will be 300 who are infected. Therefore, the other 9,700 will are healthy.</p>\n",
    "<p>Since the test has 95% sensitivity, the number of infected people identified by the test is 0.95 x 300 = 285 </p>\n",
    "<p>Therefore, among the 300 infected people, there will be 300-285 =15 who are infected but have a Healthiness in Test result.</p>\n",
    "<p>Since the test has 90% specificity, the number of healthy people identified by thhe test is 0.90 x 9700 = 8730 </p>\n",
    "<p>As total number of healthy people are 9700.Out of which 8730 are identified healthy by the test. The other 9700-8730=970 got wrong test result as the 'test Identified Infection' </p>\n",
    "<p>The Total Number of people have their test result identifying infection are 285+970=1255 </p>\n",
    "<p>In this scenario P(x) is the unconditional probability of Infected people; here it is 300/10,000 = 0.03.</p>\n",
    "<p>P(y) is the unconditional probability of Test Identifying Infection(Testing Positive); here it is 1255/10,000 = 0.1255.</p>\n",
    "<p>P(z) is the unconditional probability of Test Identifying Healthiness(Testing negative); here it is 8745/10,000 = 0.8745.</p>\n",
    "<p>Given p(z/x)=0.95 </p>\n",
    "<p>from the table</p>\n",
    "$$p(z\\mid x)=\\frac{15}{300}$$\n",
    "$$ p(z\\mid x)=0.05$$\n",
    "**we want to find** \n",
    "  <p>   (a) P(Infected|Test positive) i.e p(x/y) </p> \n",
    "  <p>   (b) P(Infected|Test Negative) i.e p(x/z) </p>\n",
    "    \n",
    "$$ p(x\\mid y) = \\frac{p(y\\mid x) \\, p(x) }{p(y)} $$\n",
    "\n",
    "$$ = \\frac{0.95 * 0.03}{0.1255} $$\n",
    "    \n",
    "$$  p(x\\mid y) = 0.2270 $$\n",
    "\n",
    "probability that a person testing positive is actually infected is 0.2770\n",
    "\n",
    "$$ p(x\\mid z) = \\frac{p(z\\mid x) \\, p(x) }{p(z)} $$\n",
    "\n",
    "$$ = \\frac{0.05 * 0.03}{0.8745} $$\n",
    "    \n",
    "$$ p(x\\mid z) = 0.00171 $$\n",
    "\n",
    "probability that a person testing negative is actually infected is 0.00171\n",
    "\n",
    "### Bayes' rule with respect to Continous random variable\n",
    "\n",
    "Let us Consider $X$,$Y$ are the two continous random variables.$f_X(x)$ is the probability density function of the variable $X$,here x is the observation and $f_Y(y)$ is the probability density function of the variable $Y$,here y is the observation\n",
    "\n",
    "$$f_{X\\mid Y}(x\\mid y) = \\frac{f_X(x)f_{Y \\mid X}(y\\mid x)}{f_Y(y)} $$\n",
    "\n",
    "$$ f_Y(y)=\\int f_X(x^{'}) f_{Y\\mid X}(y \\mid x^{'})$$\n",
    "\n",
    "\n",
    "$f_{X\\mid Y}(x\\mid y)$ is the probability density function of X when Yis true or given. \n",
    "\n",
    "$f_{Y\\mid X}(y\\mid x)$ is the probability density function of Y when X is true or given.\n",
    "\n",
    "we know that joint density function of X,Y is the probability density function of both X and Y.\n",
    "\n",
    "$f_{X,Y}(x,y)$ is the joint probability and it can be written as\n",
    "\n",
    "$$f_{X,Y}(x,y)=f_{Y \\mid X}(y \\mid x)f_X(x)=f_{X \\mid Y}(x \\mid y)f_Y(y)$$\n",
    "\n",
    "### Bayes' rule with respect to discrete random variable\n",
    "\n",
    "Let us Consider $X$,$Y$ are the two discrete random variables.$p_X(x)$ is the probability Mass function of the variable $X$,here x is the observation and $p_Y(y)$ is the probability mass function of the variable $Y$,here y is the observation\n",
    "\n",
    "$$p_{X\\mid Y}(x\\mid y) = \\frac{p_X(x)p_{Y \\mid X}(y\\mid x)}{p_Y(y)} $$\n",
    "\n",
    "$$ p_Y(y)=\\sum_{x^{'}} p_X(x^{'}) p_{Y\\mid X}(y \\mid x^{'})$$\n",
    "\n",
    "\n",
    "$p_{X\\mid Y}(x\\mid y)$ is the probability mass function of X when Yis true or given. \n",
    "\n",
    "$p_{Y\\mid X}(y\\mid x)$ is the probability mass function of Y when X is true or given.\n",
    "\n",
    "we know that joint Mass function of X,Y is the probability Mass function of both X and Y.\n",
    "\n",
    "$p_{X,Y}(x,y)$ is the joint mass function and it can be written as\n",
    "\n",
    "$$p_{X,Y}(x,y)=p_{Y \\mid X}(y \\mid x)p_X(x)=p_{X \\mid Y}(x \\mid y)p_Y(y)$$\n",
    "\n",
    "#### Example:\n",
    "Let us assume a digital communication system which sends input 1's and 0's .At the transmitter the 0's are converted into -1's before sending the signal .After this the signal is transmitted.K is the input given from the transmitter.\n",
    "\n",
    "Before the signal from the transmitter reach the receiver some noise N is added to it.Now the input and Noise are received by the detector at the receiver.Let Y be the output Signal.\n",
    "\n",
    "Let K,N,Y be the random variables of input,noise and output.\n",
    "\n",
    "As we know output=input+noise i.e Y=K+W. let us assume $W \\sim \\mathcal{N}(0,1)$\n",
    "\n",
    "what is the Probability that K=1,given that Y=y ?.....[3]\n",
    "\n",
    "Below Figure ......(updated from lecture)\n",
    "<Img src=\"Example_of_Communication.jpg\">\n",
    "\n",
    "##### Solution : \n",
    "\n",
    "The input from K can be either 1 or -1.\n",
    "so $p_K(k)=\\frac{1}{2}$\n",
    "\n",
    "Probability that K=1,given that Y=y is $p_{K \\mid Y}(1 \\mid y)$\n",
    "\n",
    "$$\\textrm { if k=1 then} \\quad p_{K \\mid Y}(1 \\mid y) \\sim \\mathcal{N}(1,1)$$\n",
    "$$\\textrm { if k=-1 then} \\quad p_{K \\mid Y}(1 \\mid y) \\sim \\mathcal{N}(-1,1)$$\n",
    "\n",
    "we know from the Bayes's Rule\n",
    "\n",
    "$$p_{K\\mid Y}(k\\mid y) = \\frac{p_K(k)f_{Y \\mid K}(y\\mid k)}{f_Y(y)} $$\n",
    "\n",
    "$$ f_Y(y)=\\sum_{k^{'}} p_K(k^{'}) f_{Y\\mid K}(y \\mid k^{'})$$\n",
    "\n",
    "$$ f_Y(y)= p_K(1) f_{Y\\mid K}(y \\mid 1)+  p_K(-1) f_{Y\\mid K}(y \\mid -1)$$\n",
    "\n",
    "$$ f_{Y\\mid K}(y \\mid k) = \\frac{1}{\\sqrt{2\\pi}} e^{\\frac{-(y-k)^2}{2}}$$\n",
    "\n",
    "$$ f_Y(y)= \\frac{1}{2} f_{Y\\mid K}(y \\mid 1)+  \\frac{1}{2} f_{Y\\mid K}(y \\mid -1)$$\n",
    "\n",
    "$$ f_Y(y)= \\frac{1}{2}.\\frac{1}{\\sqrt{2\\pi}}e^{\\frac{-(y-1)^2}{2}}  + \\frac{1}{\\sqrt{2\\pi}}e^{\\frac{-(y+1)^2}{2}}$$\n",
    "\n",
    "$$ f_Y(y)= \\frac{e^{\\frac{-(y-1)^2}{2}}}{2\\sqrt{2\\pi}} (1+e^{-2y})$$\n",
    "\n",
    "$$p_{K\\mid Y}(1\\mid y) = \\frac{p_K(1)f_{Y \\mid K}(y\\mid 1)}{f_Y(y)} $$\n",
    "\n",
    "$$p_{K\\mid Y}(1\\mid y) =  \\frac{e^{\\frac{-(y-1)^2}{2}}}{2\\sqrt{2\\pi}.{f_Y(y)}} $$\n",
    "\n",
    "$$p_{K\\mid Y}(1\\mid y) =  \\frac{e^{\\frac{-(y-1)^2}{2}}}{2\\sqrt{2\\pi}}. \\frac{2\\sqrt{2\\pi}}{e^{\\frac{-(y-1)^2}{2}}} .\\frac{1}{1+e^{-2y}}  $$\n",
    "\n",
    "$$p_{K\\mid Y}(1\\mid y) = \\frac{1}{1+e^{-2y}}  $$\n",
    "\n",
    "The Probability that K=1,given that Y=y is $\\frac{1}{1+e^{-2y}}$\n",
    "\n",
    "### Bayesian inference and the posterior distribution\n",
    "\n",
    "In this inference,$\\Theta$ is the unknown quantity of interest.It is modeled as a finite collection of random variables or as a random variable.Generally $\\Theta$ here represent physical quantities like position,velocity of vehicle or set of unknown parameters of a probabilistic model.Unless its stated explicitly we consider $\\Theta$ as a single random variable\n",
    "\n",
    "Here our aim is to find the information about $\\Theta$ by observing a collection $X=(X_1,....X_n)$ of related random variables, called observations or measurements or observation vector.To do this we assume some things like we know \n",
    "i) joint distribution of $\\Theta$ and $X$ \n",
    "ii)Prior distribution $p_\\Theta$ for discrete and $f_\\Theta$ for continuous\n",
    "iii)Conditional distribution $p_{X \\mid \\Theta}$ for discrete and $f_{X \\mid \\Theta}$ for continuous\n",
    "\n",
    "here $\\Theta$ is unknown and it is treated as random variable.\n",
    "prior distribution $p_\\Theta$ for discrete and $f_\\Theta$ for continuous comes from symmetry, known range, earlier studies, subjective or arbitrary.\n",
    "\n",
    "After observing the value of x of X ,the answer for the problem is provided by the posterior distribution of the bayesian interface which is $p_{\\Theta \\mid X}(\\theta \\mid x)$ for discrete and $f_{\\Theta \\mid X}(\\theta \\mid x)$ for discrete of $\\Theta$.\n",
    "\n",
    "The posterior distribution id calculated by using the bayes's rule.It binds complete information of $\\Theta$ ,given available information.This is the starting point for the further analysis like Point Estimates,Error Analysis etc.....[1]\n",
    "\n",
    "\n",
    "#### Steps in Bayesian Inference model\n",
    "\n",
    "**step 1**: It starts with the joint distribution of the parameters unkown $\\Theta$ and Observation X.\n",
    "or\n",
    "The prior distribution  $p_\\Theta$ or  $f_\\Theta$ and the conditional distribution $p_{X \\mid \\Theta}$ (PMF) or $f_{X \\mid \\Theta}$ (PDF)\n",
    "\n",
    "**Step 2**: As the value x of the observation is given by the Observation process the posterior distribution $p_{\\Theta \\mid X}(\\theta \\mid x)$ (PMF) or $f_{\\Theta \\mid X}(\\theta \\mid x)$ (PDF)is calculated by using Bayes' rule.\n",
    "\n",
    "**Step3**: The output from the posterior distribution can be used for further analysis, Estimates or associated probabilities or error variaces.\n",
    "\n",
    "** Summary of Bayesian Inference Model**\n",
    "Below figure.......(Copied from [3])\n",
    "\n",
    "<img src=\"Summary_of_Bayes'_Inference.PNG\">\n",
    "\n",
    "#### summary of Bayesian Inference\n",
    "Prior distribution $p_{\\Theta}$ or $_{\\Theta}$  is calculated for random variable $\\Theta$ which is unknown\n",
    "\n",
    "Using a model $p_{\\Theta \\mid X}$ or $f_{\\Theta \\mid X}$ w.r.t Observation Vector X.\n",
    "\n",
    "Once we observe the value of x of X , we calculate Posterior distribution of $\\Theta$ by using the Bayes' rule.\n",
    "\n",
    "\n",
    "As there are four versions of Bayes' we should use appropriate version to find $p_{\\Theta \\mid X}(. \\mid X = x)$ ( posterior distribution)\n",
    "\n",
    "The ouput of the Bayesian inference is Posterior distribution \n",
    "$p_{\\Theta \\mid X}(. \\mid X = x)$ (PMF) or $f_{\\Theta \\mid X}(. \\mid X = x)$ (PDF)\n",
    "\n",
    "**Ouput of Bayesian Inference**\n",
    "Below figure.......(Copied from [3])\n",
    "<img src=\"Output_of_Bayes'_Inference.PNG\">\n",
    "\n",
    "\n",
    "#### Versions of Bayes' Rule\n",
    "There are four different versions of Bayes' rules because we are going to take two inputs $\\Theta$ $X$ for different combinations like \n",
    "\n",
    "i) $\\Theta$, $X$ are discrete\n",
    "\n",
    "ii)$\\Theta$ is discrete and $X$ is continuous\n",
    "\n",
    "iii)$\\Theta$ is continuous and $X$ is discrete\n",
    "\n",
    "iv)$\\Theta$ , $X$ are continuous\n",
    "\n",
    "All four versions are syntactically similar.Generally here we are taking PDF for continuous random variable and changing it into PMF for discrete random variable\n",
    "\n",
    "<li>The four versions of Bayes' Rule are \n",
    "<ul>\n",
    "<li>$\\Theta$, $X$ are discrete:\n",
    "\n",
    "$$p_{\\Theta \\mid X}(\\theta \\mid x) = \\frac{p_{\\Theta}(\\theta)p_{X \\mid \\Theta}(x\\mid \\theta)}{\\sum_{\\theta^{'}}p_\\Theta(\\theta^{'}) P_{X\\mid \\Theta}(x \\mid \\theta^{'})} $$</li>\n",
    "\n",
    "<li>$\\Theta$ is discrete and $X$ is continuous\n",
    "\n",
    "$$p_{\\Theta \\mid X}(\\theta \\mid x) = \\frac{p_{\\Theta}(\\theta)f_{X \\mid \\Theta}(x\\mid \\theta)}{\\sum_{\\theta^{'}}p_\\Theta(\\theta^{'}) f_{X\\mid \\Theta}(x \\mid \\theta^{'})} $$</li>\n",
    "\n",
    "<li>$\\Theta$ is continuous and $X$ is discrete\n",
    "\n",
    "$$f_{\\Theta \\mid X}(\\theta \\mid x) = \\frac{f_{\\Theta}(\\theta)p_{X \\mid \\Theta}(x\\mid \\theta)}{\\int_{\\theta^{'}}f_\\Theta(\\theta^{'}) P_{X\\mid \\Theta}(x \\mid \\theta^{'})} $$</li>\n",
    "\n",
    "<li>$\\Theta$ , $X$ are continuous\n",
    "\n",
    "$$f_{\\Theta \\mid X}(\\theta \\mid x) = \\frac{f_{\\Theta}(\\theta)f_{X \\mid \\Theta}(x\\mid \\theta)}{\\int_{\\theta^{'}}f_\\Theta(\\theta^{'}) f_{X\\mid \\Theta}(x \\mid \\theta^{'})} $$</li>\n",
    "\n",
    "</ul>\n",
    "</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate Normal Distributions\n",
    "\n",
    "Generally Gaussian random variable is expressed as $\\mathcal{N}(\\mu, \\sigma^2)$ Which is also known as *Univariate normal*.As 'uni' means one univariate normal means one variable. Univariate normal distribution is the probability distribution of the normal distribution.\n",
    "\n",
    "As univariate means only one variable, Multivariate means multiple variables.So Multivariate normal distribution means representing a normal distributions in multiple dimensions.for univariate normal distribution as it considers only one variable which shows only one dimension we have one mean but in case of multivariate we use multiple variables showing different dimensions.here different variables may be position, velocity and acceleration for aircraft or production of milk and feed rate at a dairy etc.so for every dimension we should have one mean i.e for $N$ dimensions, we need $N$ means, and these can be arranged in a column matrix form like \n",
    "\n",
    "$$\n",
    "\\mu = \\begin{bmatrix}\\mu_1\\\\\\mu_2\\\\ \\vdots \\\\\\mu_n\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Let us consider the example of two dimension normal variable i.e two variables X,Y and $x = 3$ , $y = 10$. then the mean is written in matrix form as\n",
    "\n",
    "$$\n",
    "\\mu = \\begin{bmatrix}3\\\\10\\end{bmatrix} \n",
    "$$\n",
    "\n",
    "\n",
    "After finding the mean the next thing we would like to calculate is variance.Even the number of variances will be N for N dimensions.If the variance of x is 5 and y is 6 then the variance matrix can be written as \n",
    "\n",
    "$$\\sigma^2 = \\begin{bmatrix}5\\\\6\\end{bmatrix}$$ \n",
    "\n",
    "Here we took the variance in general case But we did not include how the variables vary relative to each other.For example if we consider the height and weight of a person, All the students have the same height have variance 0 and incase of different heights the variance will be large.\n",
    "\n",
    "This is incomplete because it does not consider the more general case. In the **Gaussians** chapter we computed the variance in the heights of students. That is a measure of how the weights vary relative to each other. If all students are the same height the variance is 0, and if their heights are wildly different the variance will be large.\n",
    "\n",
    "Generally the person who is taller weights more than the person who is shorter.In multivariate we not only express height and weight in term of mean, variace, By this we can say that both height and weight are correlatd.Along with the mean,variance We also need to find the degree of correlation\n",
    "\n",
    "There is also a relationship between height and weight. In general, a taller person weighs more than a shorter person.  Height and weight are *correlated*. We want a way to express not only what we think the variance is in the height and the weight, but also the degree to which they are correlated. In other words, we want to know how weight varies compared to the heights. We call that the *covariance*. \n",
    "\n",
    "One should have the  idea of the covariance and correlation befoe we undertand the multi variate normal distributions\n",
    "\n",
    "### Correlation and covariance of Multivariate disribution function\n",
    "\n",
    "In general variance is how much the data w.r.t Variable varies from each other.It is for one variable.\n",
    "covariance is how much the variables vary from each other.It is for two or more variables.\n",
    "Regarding Correlation if the data of one variable increases as the other variable increases then those two variables are positively correlated.if one variable decreases as the other variable increases then they are negatively correlated and if one variable has no effect on other then they are uncorrelated or independent.\n",
    "\n",
    "Correlation means prediction.By using the correlation we are predicting how much the changes in one variable had the effect on another variable.......[5]\n",
    "\n",
    "Here correlation means we are talking only about linear correlation.\n",
    "The Covariance of two random variables $X$ and $Y$ can be expressed as\n",
    "\n",
    "$$ COV(X, Y) = \\sigma_{xy} = \\mathbb E\\big[(X-\\mu_x)(Y-\\mu_y)\\big]$$\n",
    "\n",
    "here $\\mathbb E[X]$ is the EXPECTED VALUE of X, which is defined as\n",
    "\n",
    "$$\\mathbb E[X] =  \\begin{cases} \\sum_{i=1}^n p_ix_i & \\mbox{for discrete}\\\\ \\int_{-\\infty}^\\infty f(x)\\, x & \\mbox{for continuous}\\end{cases}$$\n",
    "\n",
    "If the probability for every data point is equal then the probability of each is $\\frac{1}{N}$, which gives\n",
    "\n",
    "$$\\mathbb E[X] =  \\frac{1}{N}\\sum_{i=1}^n x_i$$\n",
    "\n",
    "In case of discrete random variable we will take:\n",
    "\n",
    "We compare the equations of covariance and variance which looks similar in general :\n",
    "\n",
    "$$\\begin{aligned}VAR(X) = \\sigma_x^2 &= \\mathbb E[(X - \\mu)^2]\\\\\n",
    "COV(X, Y) = \\sigma_{xy} &= \\mathbb E\\big[(X-\\mu_x)(Y-\\mu_y)\\big]\\end{aligned}$$\n",
    "\n",
    "We know that $cov(X,X)$ is $var(X)$\n",
    "\n",
    "We use a *covariance matrix* to denote covariances of a multivariate normal distribution, and it looks like this:\n",
    "Generally COVARIANCE MATRIX of a multivariate normal distribution denotes covariances and the covariance matrix can be written as\n",
    "$$\n",
    "\\Sigma = \\begin{bmatrix}\n",
    "  \\sigma_1^2 & \\sigma_{12} & \\cdots & \\sigma_{1n} \\\\\n",
    "  \\sigma_{21} &\\sigma_2^2 & \\cdots & \\sigma_{2n} \\\\\n",
    "  \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n",
    "  \\sigma_{n1} & \\sigma_{n2} & \\cdots & \\sigma_n^2\n",
    " \\end{bmatrix}\n",
    "$$\n",
    "All the diagonal elements are the variances of each variable, and all the elements other than them shows covariance between the $i^{th}$ and $j^{th}$ variables.here $\\sigma_4^2$ is the variance of the fourth variable, and $\\sigma_{23}$ is the covariance between the second and third variables.\n",
    "\n",
    " If two variables are uncorrelated then the covariance between them is 0.\n",
    " \n",
    "#### covariance matrix examples\n",
    "\n",
    "**example 1** :Let us Assume the variance of $x$ and $y$ are 5 and 3 and there is no correlation between them then the covariance matrix will be \n",
    "\n",
    "\n",
    "$$\\Sigma = \\begin{bmatrix}5&0\\\\0&2\\end{bmatrix}$$\n",
    "\n",
    "**example 2** :for small amount of positive correlation between x and y the covariance matrix will be like\n",
    "\n",
    "$$\\Sigma = \\begin{bmatrix}5&0.6\\\\0.6&2\\end{bmatrix}$$\n",
    "\n",
    "here 0.6 is the covariance between $x$ and $y$. correlation is SMALL because the covariance is 0.6 which is small relative to the variances of 5. \n",
    "\n",
    "**example 3** :For a large amount of negative correlation between x and y the covariance matrix will be\n",
    "\n",
    "$$\\Sigma = \\begin{bmatrix}5&-4.7\\\\-4.7&2\\end{bmatrix}$$\n",
    "\n",
    "As we know that the covariance matrix is symmetric, the covariance between $x$ and $y$ and the covariance between $y$ and $x$ will be equal everytime.That means, $\\sigma_{xy}=\\sigma_{yx}$ for any $x$ and $y$.\n",
    "\n",
    "#### Example of gaussian distribution of one variable\n",
    "Imagine a gaussian distribution with variabe H having values H=[2.0, 1.8, 1.9, 1.7, 1.6] then we compute variance by using this method.....[5]\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\mathit{VAR}(H) &= E[(H - \\mu_H)^2] \\\\\n",
    "&= \\frac{1}{N}\\sum_{i=1}^n (H_i - \\mu_H)^2 \\\\\n",
    "&= \\frac{1}{5}\\left[(2-1.8)^2 + (1.8-1.8)^2 + (1.9-1.8)^2 + (1.7-1.8)^2 + (1.6-1.8)^2\\right] \\\\\n",
    "&= 0.02\n",
    "\\end{aligned}$$\n",
    "\n",
    "#### Example of Multivariable gaussian distribution of two variables\n",
    "Now let us consider a random variable W = [91.2, 70.1, 93.2, 59.5, 53.5]. Now if we want to write covariance matrix then....[5]\n",
    "\n",
    "$$\\Sigma = \\begin{bmatrix}\\sigma_H^2 & \\sigma_{H,W} \\\\\n",
    "\\sigma_{W,H} & \\sigma_{W}^2\\end{bmatrix}$$\n",
    "\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\mu_W &= \\frac{1}{5}(91.2 + 70.1 + 93.2 + 59.5 + 53.5) = 73.5 \\\\\n",
    "\\sigma_W^2 &= \\frac{1}{5}\\left[(91.2-73.5)^2 + (70.1-73.5)^2 + (93.2-73.5)^2 +  (59.5-73.5)^2 + (53.5-73.5)^2\\right] \\\\\n",
    "&= 261.8\n",
    "\\end{aligned}$$\n",
    "\n",
    "Covariance can be calculated by using the formula:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\sigma_{H,W} &= \\mathbb E\\big[(H-\\mu_H)(W-\\mu_W)\\big] \\\\\n",
    "&= \\frac{1}{N}\\sum_{i=1}^n (H_i-\\mu_H)(W_i-\\mu_W) \\\\\n",
    "&= (2-1.8)(91.2-73.5) + (1.8-1.8)(70.1-73.5) + (1.9-1.8)(93.2-73.5)\\, +\\\\\n",
    "&\\, \\, \\, \\,  \\, (1.7-1.8)(59.5-73.5) + (1.6-1.8)(53.5-73.5) \\\\\n",
    "&= 2.18\n",
    "\\end{aligned}$$\n",
    "\n",
    "### Multivariate Normal Distribution Equation\n",
    "\n",
    "The multivariate normal distribution in $n$ dimensions can be expressed as\n",
    "\n",
    "$$\n",
    "f(\\mathbf{x},\\, \\mu,\\,\\Sigma) = \\frac{1}{\\sqrt{(2\\pi)^n|\\Sigma|}}\\, \\exp  \\Big [{ -\\frac{1}{2}(\\mathbf{x}-\\mu)^\\mathsf{T}\\Sigma^{-1}(\\mathbf{x}-\\mu) \\Big ]}\n",
    "$$\n",
    "\n",
    "This is in the same form as the univariate normal distribution which is:\n",
    "\n",
    "$$ \n",
    "f(x, \\mu, \\sigma) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp \\Big [{-\\frac{1}{2}}{(x-\\mu)^2}/\\sigma^2 \\Big ]\n",
    "$$\n",
    "\n",
    "If X,H are two random variables then with respect to multivariate normal distribution equation then \n",
    "\n",
    "$$\n",
    "\\mu = \\begin{bmatrix}{\\mu}_H\\\\{\\mu}_W\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "X = \\begin{bmatrix}H\\\\W\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\\Sigma = \\begin{bmatrix}\\sigma_H^2 & \\sigma_{H,W} \\\\\n",
    "\\sigma_{W,H} & \\sigma_{W}^2\\end{bmatrix}$$\n",
    "\n",
    "\n",
    "$$\n",
    "X - \\mu = \\begin{bmatrix}{H - \\mu_x}\\\\{W - \\mu_w}\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "[ X - \\mu ]^T= \\begin{bmatrix}{H - \\mu_x} \\quad {W - \\mu_w}\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "### plots of covariance matrix\n",
    "\n",
    "Below figue.....(Copied from [5])\n",
    "<img src=\"Plot_of_covariance_matrix_1.PNG\">\n",
    "\n",
    "\n",
    "\n",
    "The above figures are the plots of covariance matrix.\n",
    "\n",
    "first one is when covariance between $X$ and $Y$  are 0 , mean of both $X$ and $Y$ are same\n",
    "\n",
    "second one is when covariance between $X$ and $Y$ are 0 , mean of both $X$ and $Y$ are different\n",
    "\n",
    "third one is when covariance between $X$ and $Y$ are not equal to 0 , mean of both $X$ and $Y$ are different\n",
    "\n",
    "Below figue.....(Copied from [5])\n",
    "<img src=\"Plot_of_covariance_matrix_2.gif\">\n",
    "\n",
    "An animation of varyig the covariance while holding the variance constant.\n",
    "\n",
    "\n",
    "### plot of probability density function of Multivariate normal distribution\n",
    "\n",
    "Below figue.....(Copied from [12])\n",
    "<img src=\"PDF_of_Multivariate.PNG\">\n",
    "                                             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Communication Example using bayes' rule:\n",
    "Let us assume a digital communication system which sends input 1's and 0's .At the transmitter the 0's are converted into -1's before sending the signal .After this the signal is transmitted.K is the input given from the transmitter.\n",
    "\n",
    "Before the signal from the transmitter reach the receiver some noise N is added to it.Now the input and Noise are received by the detector at the receiver.Let Y be the output Signal.\n",
    "\n",
    "Let K,N,Y be the random variables of input,noise and output.\n",
    "\n",
    "As we know output=input+noise i.e Y=K+W. let us assume $W \\sim \\mathcal{N}(0,1)$\n",
    "\n",
    "what is the Probability that K=1,given that Y=y ?.....[3]\n",
    "\n",
    "<img src=\"Example_of_Communication.jpg\">\n",
    "\n",
    "\n",
    "\n",
    "### Solution : \n",
    "\n",
    "The input from K can be either 1 or -1.\n",
    "so $p_K(k)=\\frac{1}{2}$\n",
    "\n",
    "Probability that K=1,given that Y=y is $p_{K \\mid Y}(1 \\mid y)$\n",
    "\n",
    "$$\\textrm { if k=1 then} \\quad p_{K \\mid Y}(1 \\mid y) \\sim \\mathcal{N}(1,1)$$\n",
    "$$\\textrm { if k=-1 then} \\quad p_{K \\mid Y}(1 \\mid y) \\sim \\mathcal{N}(-1,1)$$\n",
    "\n",
    "we know from the Bayes's Rule\n",
    "\n",
    "$$p_{K\\mid Y}(k\\mid y) = \\frac{p_K(k)f_{Y \\mid K}(y\\mid k)}{f_Y(y)} $$\n",
    "\n",
    "$$ f_Y(y)=\\sum_{k^{'}} p_K(k^{'}) f_{Y\\mid K}(y \\mid k^{'})$$\n",
    "\n",
    "$$ f_Y(y)= p_K(1) f_{Y\\mid K}(y \\mid 1)+  p_K(-1) f_{Y\\mid K}(y \\mid -1)$$\n",
    "\n",
    "By using guassian distribution $$ f_{Y\\mid K}(y \\mid k) = \\frac{1}{\\sqrt{2\\pi}} e^{\\frac{-(y-k)^2}{2}}$$\n",
    "\n",
    "$$ f_Y(y)= \\frac{1}{2} f_{Y\\mid K}(y \\mid 1)+  \\frac{1}{2} f_{Y\\mid K}(y \\mid -1)$$\n",
    "\n",
    "$$ f_Y(y)= \\frac{1}{2}.\\frac{1}{\\sqrt{2\\pi}}e^{\\frac{-(y-1)^2}{2}}  + \\frac{1}{\\sqrt{2\\pi}}e^{\\frac{-(y+1)^2}{2}}$$\n",
    "\n",
    "$$ f_Y(y)= \\frac{e^{\\frac{-(y-1)^2}{2}}}{2\\sqrt{2\\pi}} (1+e^{-2y})$$\n",
    "\n",
    "$$p_{K\\mid Y}(1\\mid y) = \\frac{p_K(1)f_{Y \\mid K}(y\\mid 1)}{f_Y(y)} $$\n",
    "\n",
    "$$p_{K\\mid Y}(1\\mid y) =  \\frac{e^{\\frac{-(y-1)^2}{2}}}{2\\sqrt{2\\pi}.{f_Y(y)}} $$\n",
    "\n",
    "$$p_{K\\mid Y}(1\\mid y) =  \\frac{e^{\\frac{-(y-1)^2}{2}}}{2\\sqrt{2\\pi}}. \\frac{2\\sqrt{2\\pi}}{e^{\\frac{-(y-1)^2}{2}}} .\\frac{1}{1+e^{-2y}}  $$\n",
    "\n",
    "$$p_{K\\mid Y}(1\\mid y) = \\frac{1}{1+e^{-2y}}  $$\n",
    "\n",
    "The Probability that K=1,given that Y=y is $\\frac{1}{1+e^{-2y}}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### python code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4xLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvAOZPmwAAIABJREFUeJzt3XuYXXV97/H3ZyaZ3DMBkhByI1zC\nJVESIAUr1ioXC7QHWkWFipeKpaiUYttzao8+iHjO81iv9YIiWuWqXLTSSONB23rXYBKYCSRBCQGy\nh0QIlz1DSCbJZL7nj7Vm2N3ZM7MzM2vf5vN6nv3sdfnttb57zZ793ev3W+v3U0RgZmYG0FTtAMzM\nrHY4KZiZWT8nBTMz6+ekYGZm/ZwUzMysn5OCmZn1c1IwM7N+TgpmZtbPScHMzPqNq3YAB2vmzJmx\naNGiaodhZlZX1q1b92xEzBqqXN0lhUWLFrF27dpqh2FmVlckPVlOOVcfmZlZPycFMzPr56RgZmb9\nnBTMzKyfk4KZmfXLLClI+rqkZyQ9PMB6Sfq8pM2S1ks6JatYzMysPFmeKdwEnDvI+vOAxenjcuDL\nGcZiZmZlyOw+hYj4qaRFgxS5ELglkvFAV0uaIemIiNieVUxmNnw9+3vZ09PLvv297O1Jpvem033L\n9vb0sqfEsr5yERAEfaMABxy4LAZfP5AhBxZugKGHzzrxcJYtmJHpPqp589o8IFcw35EuOyApSLqc\n5GyChQsXViQ4s7Esv2svG7Z1sWFbZ/rcxZYdO+mt8+9VqdoRjMzs6RMbOimU+vOU/MhFxI3AjQAr\nVqyo84+lWe2ICLZ3dv+3BLBxWxdP5Xf3lzmidSJL507n3KVzmDZxHC3jmmgZ18T45iYmjGuipfnl\n+b51Lem6wmXjm5toEkjq/+eXQKj/y/qAeV4ur/S1lq1qJoUOYEHB/HxgW5ViMWt4+3uDx599iQ3b\nOtmY/vrfsK2TF3btA5Iv3aNmTuGUIw/h7b9/JEvnTmfp3FYOndJS5citkqqZFFYCV0q6Azgd6HR7\nglk21j35Au+5eU1/AmhpbuK4OVN5w5I5LJ03naVzp3PCnOlMmVB33aHZKMvsEyDpW8DrgJmSOoCP\nAOMBIuIGYBVwPrAZ2AX8RVaxmI1lT3d1c8Vt65g+aTz/+/wTWTq3lWNnT6VlnG9TsgNlefXRJUOs\nD+D9We3fzGBPz36uuG0dL+3p4bbLTuf4OdOqHZLVOJ8rmjWwa1du4MGteb78tlOcEKwsPn80a1C3\n3/8k3/p1jve//hjOe+UR1Q7H6oSTglkDWvvE81y7cgOvO34Wf3vO8dUOx+qIk4JZg3m6q5v33v4A\n82ZM4nMXn0xzk6/tt/K5TcGsgfQ1LO/a08Pt7zmd1knjqx2S1RknBbMGERF85N+ShuUbLj2F4w53\nw7IdPFcfmTWI2+/fyh1rclz5+mM59xVuWLbhcVIwawBrn3iej35vA68/fhYfOOe4aodjdcxJwazO\n/a6zmytue4D5h0zmn92wbCPkNgWzOtbXsLx7bw/f+ks3LNvIOSmY1amI4Jp7NtCWy3PDpaey2A3L\nNgpcfWRWp267fyt3rs3x12cey7mvmFPtcKxBOCmY1aE1TzzPR1du4MwTZvOBs92wbKPHScGszmzv\n3M17b3uABYdO5rNvXU6TG5ZtFLlNwayOdO/bzxW3PeCGZcuMk4JZnYgIrvm3h2nP5fnK292wbNlw\n9ZFZnbht9ZPctbaDq848lj9a6oZly4aTglkd2LJjJx/93kbOOmE2V7th2TLkpGBWB36x+Vl6eoNr\nL1jqhmXLlJOCWR1oy3Uyc+oE5h8yqdqhWINzUjCrA225F1i+oBXJZwmWLScFsxrX1b2Px3a8xLL5\nM6odio0BTgpmNW59rhOA5QudFCx7TgpmNa69Iw/AST5TsApwUjCrcQ9uzXP0rCm+e9kqwknBrIZF\nBG25PMt9lmAV4qRgVsO2dXbz7M49bk+winFSMKth7bmkPcFXHlmlOCmY1bC2XJ6W5iZOPGJ6tUOx\nMcJJwayGteXyLJk7nZZx/le1ysj0kybpXEm/kbRZ0gdLrF8o6UeSHpS0XtL5WcZjVk969vfyUEcn\nyxe46sgqJ7OkIKkZuB44D1gCXCJpSVGxDwN3RcTJwMXAl7KKx6zePPrMTnbv2++kYBWV5ZnCacDm\niNgSEXuBO4ALi8oE0FdZ2gpsyzAes7rSljYyOylYJWU58to8IFcw3wGcXlTmWuAHkv4amAKcnWE8\nZnWlPZdnxuTxHHnY5GqHYmNIlmcKpbpzjKL5S4CbImI+cD5wq6QDYpJ0uaS1ktbu2LEjg1DNak9b\nLs+y+TPcM6pVVJZJoQNYUDA/nwOrhy4D7gKIiF8BE4GZxRuKiBsjYkVErJg1a1ZG4ZrVjpf29PDb\np19kmauOrMKyTAprgMWSjpLUQtKQvLKozFbgLABJJ5IkBZ8K2Jj30FOd9Aac7KRgFZZZUoiIHuBK\n4D5gE8lVRhskXSfpgrTY3wF/Kakd+BbwrogormIyG3P67mQ+aX5rlSOxsSbLhmYiYhWwqmjZNQXT\nG4EzsozBrB615fIsPHQyh02dUO1QbIzxbZJmNag9l3d7glWFk4JZjXmmq5ttnd2+P8GqwknBrMa8\nfNOa2xOs8pwUzGpMe0eecU1i6VwnBas8JwWzGtOWy3PCEdOYOL652qHYGOSkYFZDenuD9blOD6pj\nVeOkYFZDtjy7kxf39LiR2arGScGshrTlOgH3jGrV46RgVkPaci8wdcI4jpk1tdqh2BjlpGBWQ9pz\nnZw0v5WmJveMatXhpGBWI7r37WfT9i5XHVlVDSspSPr+aAdiNtZt2NZFT2+4ewurqgE7xJN0ykCr\ngOXZhGM2dvXdyezusq2aBusldQ3wE0qPoOZPrdkoa8/lOaJ1IrOnT6x2KDaGDZYUNgF/FRGPFq+Q\nlCtR3sxGoC2Xd3uCVd1gbQrXDrL+r0c/FLOx6/mX9rL1+V1uT7CqG/BMISK+Pci6e7IJx2xsau/v\nGdVJwaqrrKuPJJ1Z+Gxmo6stl6dJ8Mp57hnVqqvcS1I/VfRsZqOoLZfnuMOnMWVCpiPkmg3pYO9T\n8G2WZqMsImjvyLtnVKsJvqPZrMqefG4X+V37WL7QScGqz0nBrMraO5JGZp8pWC1wUjCrsge35pk0\nvpnjDnfPqFZ95SaFnenzi1kFYjZWtXfkeeW8VsY1+zeaVV9Zn8KIeG3hs5mNjr09vWzY1sWyBb4U\n1WrDkElB0mUlln08m3DMxpZHftfF3p5eli84pNqhmAGD933U5yJJ3RFxO4CkLwETsg3LbGzo6xnV\nZwpWK8pJCm8EVkrqBc4Dno+I92UbltnY0JbLM3PqBObNmFTtUMyAwcdTOLRg9j3APcAvgOskHRoR\nz2cdnFmjS3pGbUXyfaFWGwY7U1gHBMldzH3Pf5w+Ajg68+jMGljn7n1s2fESbzx5XrVDMes3WC+p\nR1UyELOx5qGOTgB3l201JdMLoyWdK+k3kjZL+uAAZd4iaaOkDZK+mWU8ZrWkLfcCACf5TmarIZl1\nySipGbgeOAfoANZIWhkRGwvKLAb+ETgjIl6QNDureMxqTVuuk6NnTaF10vhqh2LWb8AzBUlnpM/D\nvfz0NGBzRGyJiL3AHcCFRWX+Erg+Il4AiIhnhrkvs7oSER5+02rSYNVHn0+ffzXMbc8DCsdy7kiX\nFToOOE7SLyStlnRuqQ1JulzSWklrd+zYMcxwzGrHts5unt25x0nBas5g1Uf7JH0DmCfp88UrI+Kq\nIbZd6hq7KLH/xcDrgPnAzyS9IiLyRfu6EbgRYMWKFcXbMKs7bVs9/KbVpsGSwp8AZwNnklyeerA6\ngAUF8/OBbSXKrI6IfcDjkn5DkiTWDGN/ZnWjvSNPy7gmTpgzvdqhmP03g12S+ixwh6RNEdE+jG2v\nARZLOgp4CrgY+POiMvcAlwA3SZpJUp20ZRj7MqsrbVvzLJ07nZZx7hnVaks5n8jnJH1X0jOSnpb0\nHUnzh3pRRPQAVwL3AZuAuyJig6TrJF2QFrsv3f5G4EfA/4yI54b5XszqQs/+Xh56qtOD6lhNKueS\n1G8A3wTenM5fmi47Z6gXRsQqYFXRsmsKpgP42/RhNib89umd7N63n5M9/KbVoHLOFGZHxDcioid9\n3ATMyjgus4bl4TetlpWTFHZIulRSc/q4FHAVj9kwtW3NM2PyeI48bHK1QzE7QDlJ4d3AW4DfAduB\ni9JlZjYM7R15ls2f4Z5RrSYN2aYQEVuBC4YqZ2ZDe2lPD799+kXesHROtUMxK8nXw5lV0ENPddIb\ncLJvWrMa5aRgVkF9w2+eNN/Db1ptGjIppL2dmtkoaM/lWXjoZA6b6mHOrTaVc6awWdInJS3JPBqz\nBteWy3tQHatp5SSFk4DfAl9LezK9XJI7bDE7SE93dbO9s9ud4FlNGzIpRMSLEfHViHg18L+AjwDb\nJd0s6djMIzRrEH3tCcsXuD3BaldZbQqSLpD0XeBzwKeBo4HvUdSFhZkNrD2XZ1yTWDrXScFqVzl9\nHz1K0lndJyPilwXLvy3ptdmEZdZ42nJ5TjhiGhPH+9oNq13ltCm8IyIuK0wIfUN1ljHQjpkBvb3B\n+o5OtydYzSsnKRww6hrwhdEOxKyRPbZjJzv39LgTPKt5A1YfSfp94NXALEmFXVtPB3z+a3YQXm5k\ndlKw2jZYm0ILMDUtM61geRdJp3hmVqb2jjzTJozjmFlTqx2K2aAGG47zJ8BPJN0UEU9WMCazhtOW\ny3PSglaamtwzqtW2waqP/jkirga+KCmK10eEe041K0P3vv08sv1FLn/t0dUOxWxIg1Uf3Zo+f6oS\ngZg1qg3bOunpDbcnWF0YrPpoXfr8k8qFY9Z42nKdgBuZrT4MVn30EHBAtVGfiDgpk4jMGkxbLs/c\n1onMnj6x2qGYDWmw6qM/qVgUZg2s3T2jWh0ZrPrIVxyZjdBzO/ew9fldvO30hdUOxawsA97RLOnn\n6fOLkrqKnysXoln9Wt+RtCf4TMHqxWBnCq9Jn6cNVMbMBvdgLk+T4JXz3DOq1YdyeklF0inAa0ga\nnn8eEQ9mGpVZg2jP5Tnu8GlMmVDWv5pZ1ZUznsI1wM3AYcBM4CZJH846MLN6FxG0d+R9KarVlXJ+\nvlwCnBwR3QCSPg48APyfLAMzq3dPPreL/K59bk+wulJO19lPAIUXWE8AHsskGrMG0tczqrvLtnoy\n2M1rXyBpQ9gDbJD0w3T+HODnlQnPrH615fJMGt/McYe7Z1SrH4NVH61Nn9cB3y1Y/uNyNy7pXJJx\nnZuBr0XExwcodxFwN/B7EbG2VBmzetOWy/PKea2May7nhNysNgx2SerNI9mwpGbgepIziw5gjaSV\nEbGxqNw04Crg/pHsz6yW7O3pZeO2Lt51xqJqh2J2UMq5+mixpG9L2ihpS9+jjG2fBmyOiC0RsRe4\nA7iwRLmPAZ8Aug8qcrMatml7F3v397o9wepOOee13wC+DPQArwdu4eVutQczD8gVzHeky/pJOhlY\nEBH3lhWtWZ1o70iH31zopGD1pZykMCki/hNQRDwZEdcCZ5bxulJDTPX3uiqpCfgs8HdDbki6XNJa\nSWt37NhRxq7Nqqtta56ZUycwt9U9o1p9KScpdKdf4I9KulLSnwGzy3hdB7CgYH4+sK1gfhrwCuDH\nkp4AXgWslLSieEMRcWNErIiIFbNmzSpj12bV1ZbetCZ5+E2rL+UkhauBySSNwacCbwfeWcbr1gCL\nJR0lqQW4GFjZtzIiOiNiZkQsiohFwGrgAl99ZPWuc/c+tux4ieUL3N+R1Z8h72iOiDXQX91zVUS8\nWM6GI6JH0pXAfSSXpH49IjZIug5YGxErB9+CWX1a39eesOCQKkdidvCGTAppdc43SKp7kNQJvLtv\nuM7BRMQqYFXRsmsGKPu6MuI1q3nt6Z3Mr5zvMwWrP+X0ffR14H0R8TMASa8hSRIejtOshLZcnmNm\nTaF10vhqh2J20MppU3ixLyEARMTPgbKqkMzGmoigLdfpTvCsbg3W99Ep6eSvJX0F+BbJJaVv5SC6\nujAbS57K7+bZnXs42UnB6tRg1UefLpr/SMF0YGYHaM95+E2rb4P1ffT6SgZi1gjaci/QMq6JE+ZM\nr3YoZsNSTt9HrZI+03dHsaRPS/JlFWYltOc6WTp3Oi3j3DOq1adyPrlfJ2lYfkv66CK5+sjMCvTs\n7+Whpzo9/KbVtXIuST0mIt5UMP9RSW1ZBWRWr3779E5279vvpGB1rZwzhd3pvQkASDoD2J1dSGb1\nqW/4TScFq2flnClcAdxS0I7wAuX1fWQ2prTn8hwyeTwLD51c7VDMhm3QpJD2d3R8RCyTNB0gIroq\nEplZnWnL5VnmnlGtzg1afRQRvcCV6XSXE4JZaTv39PDbZ170SGtW98ppU/ihpL+XtEDSoX2PzCMz\nqyMPdXQS4ZHWrP6V06bw7vT5/QXLAjh69MMxq099w2/6TMHqXTnjKRxViUDM6lnb1jxHHjaZQ6e0\nVDsUsxEpZzyFicD7gNeQnCH8DLghIrozjs2sbrR35Pm9Ra5VtfpXTvXRLSR3NH8hnb8EuBV4c1ZB\nmdWTp7u62d7Z7U7wrCGUkxSOj4hlBfM/ktSeVUBm9cY3rVkjKefqowclvapvRtLpwC+yC8msvrTn\n8oxrEkvnumdUq3/lnCmcDrxD0tZ0fiGwSdJDQESEh+W0Ma0tl+fEI6YzcXxztUMxG7FyksK5mUdh\nVqd6e4P1HZ386clzqx2K2ago55LUJysRiFk9emzHTnbu6WH5gkOqHYrZqPBIIGYj8HIjs8edssbg\npGA2Am25PNMmjOPomVOrHYrZqHBSMBuB9o48Jy1opanJPaNaY3BSMBum7n37eWT7i74/wRqKk4LZ\nMG3Y1klPb7gTPGsoTgpmw/TgVt/JbI3HScFsmNo7OpnbOpHZ0ydWOxSzUeOkYDZMbbkXPKiONRwn\nBbNheG7nHnLP73Z7gjWcTJOCpHMl/UbSZkkfLLH+byVtlLRe0n9KOjLLeMxGS99Ia25PsEaTWVKQ\n1AxcD5wHLAEukbSkqNiDwIq0U71vA5/IKh6z0dSW66RJ8Ip5vpPZGkuWZwqnAZsjYktE7AXuAC4s\nLBARP4qIXensamB+hvGYjZq2XJ7jDp/GlAnl9ClpVj+yTArzgFzBfEe6bCCXAd8vtULS5ZLWSlq7\nY8eOUQzR7OBFBO25vKuOrCFlmRRK3fcfJQtKlwIrgE+WWh8RN0bEiohYMWvWrFEM0ezgPfHcLjp3\n73NSsIaU5blvB7CgYH4+sK24kKSzgQ8BfxgRezKMx2xUtKc9o3pMZmtEWZ4prAEWSzpKUgtwMbCy\nsICkk4GvABdExDMZxmI2atpyeSa3NHPc4dOqHYrZqMssKURED3AlcB+wCbgrIjZIuk7SBWmxTwJT\ngbsltUlaOcDmzGpGWy7PK+a10uyeUa0BZXrpRESsAlYVLbumYPrsLPdvNtr29vSycVsXf3HGomqH\nYpYJ39FsdhA2be9i7/5etydYw3JSMDsIvpPZGp2TgtlBaNuaZ9a0CRzR6p5RrTE5KZgdhLaOPMvm\nz0ByI7M1JicFszJ17trHlh0vcbK7y7YG5qRgVqb1T6U3rbm7bGtgTgpmZWpLh988aYF7RrXG5aRg\nVqb2jjzHzJrC9Injqx2KWWacFMzKEBG05fIsX3BItUMxy5STglkZnsrv5tmde1nuqiNrcE4KZkOI\nCD73H48CcNpRh1U5GrNsOSmYDeHW1U9y97oOrjrzWI6f455RrbE5KZgN4v4tz3Hd9zZy1gmzufrs\n46odjlnmnBTMBrAtv5v3f/MBFh46mc9evJwmd5VtY4BHHTcroXvfft572zq69/Vyx+Wn+jJUGzOc\nFMyKRAQfvudh2js6ufHtp3LsbLcj2Njh6iOzIreufpJvr+vgqrMW84alc6odjllFOSmYFehrWD77\nxNlcfdbiaodjVnFOCmap/oblwybzmbe6YdnGJrcpmFHcsLzCDcs2Zjkp2JhX2LD81Xes4NjZU6sd\nklnVuPrIxrxbfpU0LP/NWYs5Z8nh1Q7HrKqcFGxMu3/Lc3zs3o2cfeLh/I0bls2cFGzs2pbfzftu\nTxqWP/vWZW5YNsNJwcao7n37ueK2dezp6eXGt69gmhuWzQA3NNsYFBF86LsPs94Ny2YH8JmCjTk3\n//IJvvNAB1ef7YZls2JOCjamrN7yHB/7902cs+RwrjrTDctmxZwUbMx4Kr+b99/+AIsOm8xn3uKG\nZbNS3KZgDW/X3h42be/i2pUb2dvTy43vcMOy2UAyTQqSzgU+BzQDX4uIjxetnwDcApwKPAe8NSKe\nyDIma2zPv7SXDds62bCtK3108vizLxEB45rEDZeeyjGz3LBsNpDMkoKkZuB64BygA1gjaWVEbCwo\ndhnwQkQcK+li4J+At2YVkzWOiOCp/O7+L/+NaSLY3tndX2bejEksmTudC5bNZencVpbNb2X29IlV\njNqs9mV5pnAasDkitgBIugO4EChMChcC16bT3wa+KEkRERnGZTVif2+wb38ve3p62dvTy979yfO+\n9Llved98V/c+Nm1Pk8D2LvK79gHQJDh61lROO+pQls6dztK5rSw5YjqHTGmp8js0qz9ZJoV5QK5g\nvgM4faAyEdEjqRM4DHh2tIO5a02Or/5sy2hvtqJGmimHyrVDbj+SMhGRPvctDiIK5gdY3xv0f8Hv\n3d/L/t6Df0ct45o4Yc40znvFHJbMbWXp3OmcOGc6k1qaD3pbZnagLJNCqUs7ir8FyimDpMuBywEW\nLlw4rGBmTB7P4sPrvy5ZJQ/ZQW1gJKuRkggk0me9/Dol8b287uV4paRsS7NoGdeUPJqbaRnXxPhm\nMaFvWbp8fEG5CemySS3NHHnYZMY3+6I5s6xkmRQ6gAUF8/OBbQOU6ZA0DmgFni/eUETcCNwIsGLF\nimH9YH7D0jkeWtHMbAhZ/uRaAyyWdJSkFuBiYGVRmZXAO9Ppi4D/cnuCmVn1ZHamkLYRXAncR3JJ\n6tcjYoOk64C1EbES+BfgVkmbSc4QLs4qHjMzG1qm9ylExCpgVdGyawqmu4E3ZxmDmZmVzy12ZmbW\nz0nBzMz6OSmYmVk/JwUzM+vnpGBmZv1Ub7cFSNoBPDnMl88kgy40RpHjGxnHN3K1HqPjG74jI2LW\nUIXqLimMhKS1EbGi2nEMxPGNjOMbuVqP0fFlz9VHZmbWz0nBzMz6jbWkcGO1AxiC4xsZxzdytR6j\n48vYmGpTMDOzwY21MwUzMxtEwyUFSW+WtEFSr6QVRev+UdJmSb+R9EcDvP4oSfdLelTSnWm331nF\neqektvTxhKS2Aco9IemhtNzarOIpsd9rJT1VEOP5A5Q7Nz2mmyV9sILxfVLSI5LWS/qupBkDlKvo\n8RvqeEiakP7tN6eftUVZx1Sw7wWSfiRpU/p/8jclyrxOUmfB3/2aUtvKMMZB/15KfD49fuslnVLB\n2I4vOC5tkrokXV1UpqrHb8QioqEewInA8cCPgRUFy5cA7cAE4CjgMaC5xOvvAi5Op28A3luhuD8N\nXDPAuieAmVU4ltcCfz9Emeb0WB4NtKTHeEmF4nsDMC6d/ifgn6p9/Mo5HsD7gBvS6YuBOyv4Nz0C\nOCWdngb8tkR8rwPurfTnrdy/F3A+8H2SAf5eBdxfpTibgd+RXP9fM8dvpI+GO1OIiE0R8ZsSqy4E\n7oiIPRHxOLAZOK2wgJKxJc8Evp0uuhn40yzjLdjvW4BvZb2vDJwGbI6ILRGxF7iD5FhnLiJ+EBE9\n6exqktH9qq2c43EhyWcLks/aWeob1zRjEbE9Ih5Ip18ENpGMlV5PLgRuicRqYIakI6oQx1nAYxEx\n3Jtpa1LDJYVBzANyBfMdHPjPcBiQL/iiKVUmC38APB0Rjw6wPoAfSFqXjlddSVemp+hfl3RIifXl\nHNdKeDfJr8dSKnn8yjke/WXSz1onyWevotJqq5OB+0us/n1J7ZK+L2lpRQMb+u9VK5+5ixn4h1w1\nj9+IZDrITlYk/QdQasDlD0XEvw30shLLii+9KqfMQSkz1ksY/CzhjIjYJmk28ENJj0TET0cSVznx\nAV8GPkZyDD5GUsX17uJNlHjtqF3SVs7xk/QhoAe4fYDNZHb8SqjK5+xgSZoKfAe4OiK6ilY/QFIl\nsjNtR7oHWFzB8Ib6e9XC8WsBLgD+scTqah+/EanLpBARZw/jZR3AgoL5+cC2ojLPkpyKjkt/wZUq\nc1CGilXSOOCNwKmDbGNb+vyMpO+SVFGMypdaucdS0leBe0usKue4DlsZx++dwJ8AZ0VaoVtiG5kd\nvxLKOR59ZTrSv38ryXC0FSFpPElCuD0i/rV4fWGSiIhVkr4kaWZEVKRPnzL+Xpl+5sp0HvBARDxd\nvKLax2+kxlL10Urg4vTKj6NIMvevCwukXyo/Ai5KF70TGOjMY7ScDTwSER2lVkqaImla3zRJ4+rD\nGcfUt+/Ceto/G2C/a4DFSq7aaiE5pV5ZofjOBf4BuCAidg1QptLHr5zjsZLkswXJZ+2/Bkpooy1t\nu/gXYFNEfGaAMnP62jgknUbyPfFcheIr5++1EnhHehXSq4DOiNheifgKDHh2X83jNyqq3dI92g+S\nL68OYA/wNHBfwboPkVwZ8hvgvILlq4C56fTRJMliM3A3MCHjeG8CrihaNhdYVRBPe/rYQFJtUqlj\neSvwELCe5B/xiOL40vnzSa5ieazC8W0mqVtuSx83FMdXjeNX6ngA15EkL4CJ6Wdrc/pZO7qCx+w1\nJFUt6wuO2/nAFX2fQ+DK9Fi1kzTgv7qC8ZX8exXFJ+D69Pg+RMFVhhWKcTLJl3xrwbKaOH6j8fAd\nzWZm1m8sVR+ZmdkQnBTMzKyfk4KZmfVzUjAzs35OCmZm1s9JwUZM0lVpr5sD3VE8km1/TdKS0d7u\nIPt7QtLMdPqXBcs/mfYq+klJs5T0bvqgpD/IMJZFkv48q+0PsL+K3ANjtasu72i2mvM+kvs+Hi+n\ncMEd40OKiPeMKLIRiIhXF8z+FTArIvZIupjkhsN3DvDSA0hqjoj9BxnCIuDPgW8e5OvMhs1nCjYi\nkm4gueFopaQPSDpU0j1pJ3qFQ9jKAAAEgklEQVSrJZ2UlrtW0o2SfgDcUrSNprQrgA2S7pW0StJF\n6bofS1oh6b2SPlHwmndJ+kI6famkXyvpu/4rkprT5Tsl/d+0Y7LVkg4vEf9hkn6Q/ur/CgX96kja\nmT6vBKYA90v6B+ATwPnp/iZJeoOkX0l6QNLdab9CfWcd10j6OfBmScdI+n9KOnr7maQT0nI3KRkf\n4JeStvS9d+DjwB+k+/lAUdy3SrqwYP52SRcUlblTBWNgpPt5U3pG8LM03gckFSa/wuP7xYL5eyW9\nLp0e6P1+XNLG9G//qeJtWp2o9t1zftT/g4L+74EvAB9Jp88E2tLpa4F1wKQSr7+I5K7yJpLO714A\nLkrX/RhYAcwi6ZK67zXfJ7k790Tge8D4dPmXgHek0wH8j3T6E8CHS+z786TjWAB/nL6m773sLChX\nOP0u4Ivp9EySfnmmpPP/ULC9J4D/VfC6/wQWp9Onk3RvAcld7Xen739J3/tkkH75gT8E7kmnW4HH\nSceWKCjzZ8DN6XQLyd3fk0juyJ2YLl8MrE2nFwEPF7/HdP7eNJ6S7xc4lKSngL4bYmdU+3Ppx/Ae\nrj6y0fYa4E0AEfFf6S/x1nTdyojYPcBr7o6IXuB3kn5UXCAidqS/ol8FPEoykNIvgPeTdCa4Ju1u\nZhLwTPqyvbzcid864JwS+34tSYeERMS/S3rhIN/vq0i+yH+R7r8F+FXB+juhv1fSVwN36+WhEyYU\nlLsnff8bS53RFIuIn0i6XklPom8EvhMHVsl9H/i8pAnAucBPI2J3+vf4oqTlwH7guFF4v11AN/A1\nSf9O6c4TrQ44KdhoG6xb45cO4jWl3EkyGNEjwHcjIpR8M90cEaW6MN4X6c9Wki+/gT7vI+nrRcAP\nI+KSAdb3vecmkrE6lg9Qbk/RNstxK/A2kk73irs0JyK6Jf0Y+CPgrbzcgdsHSPoFW5bG1V1i2z38\n9+rliQWxlXy/Sjp/OyuN50qSM0WrM25TsNH2U5IvKtI66GfjwP76i/0ceFPatnA4STVFKf9KMhLe\nJaS/wEmqZC5KfzGTtmkcOcx4zwNKDSQ0mNXAGZKOTbcxWdIBv7zTY/C4pDen5SRp2RDbfpFkyMyB\n3ARcnW5/wwBl7gD+gmQgp/vSZa3A9vTM5O0kw0oWewJYnv5NFvDyKIUl3296JtQaEavSmAZKflbj\nnBRstF0LrJC0nqShtJwrdL5D0rPtw8BXSEYC6ywuFBEvABtJBjD5dbpsI/BhkpG61gM/JBmHuFwf\nBV4r6QGSbpq3HsRriYgdJPXv30r3vxo4YYDibwMuk9TXA+hQw5auB3rShvIPFK+MpC//TcA3BtnG\nD0iqyP4jkuFBIWl3eaek1SRVR6XO4H5B0k7xEPApkoFjBnu/04B702U/ITkbsTrkXlKtJkiaGslI\nVYeRdCd9RkT8rtpx1TJJk0m+tE+JiAOSqNlwuE3BasW9kmaQNFx+zAlhcJLOBr4OfMYJwUaTzxTM\nzKyf2xTMzKyfk4KZmfVzUjAzs35OCmZm1s9JwczM+jkpmJlZv/8PY1elczrezPAAAAAASUVORK5C\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2aec3573470>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt        ## importing matplotlib to plot the graph\n",
    "import numpy as np \n",
    "import math                            ## importing math to calculate exponential and square root\n",
    "k = np.array([1, -1])                  ## two imput signals of k 1 and -1\n",
    "prob_k=np.array([1/2, 1/2])            ## probability of occurence of k when it is 1 and -2\n",
    "def prob_y_givenk(k,y):                ## function that calculates probability density function of y for given k\n",
    "    prob=(math.exp((-math.pow(y-k,2))/2))/(math.sqrt(2*math.pi))\n",
    "    return prob\n",
    "def proby(k,y):                        ## function that calculates probability density function of k\n",
    "    prob_y=k[0]*prob_y_givenk(1,y)+k[1]*prob_y_givenk(-1,y)\n",
    "    return prob_y\n",
    "y= np.arange(-10,10,1)                 ## taking a set of values for y\n",
    "prob_k1_given_y_set = []               ## array for storing probability of k=1 for given values of y\n",
    "for i in range(0, len(y)):\n",
    "    prob_k1_given_y = (prob_y_givenk(1,y[i])*prob_k[0])/proby(prob_k,y[i])     ## probability of k=1 for given y (Bayes' theorem)\n",
    "    prob_k1_given_y_set.append(prob_k1_given_y)           ## storing the probability of k=1 for specific y in a array\n",
    "plt.plot(y, prob_k1_given_y_set)                   ## plotting the graph between probability of k=1 for some given values of y\n",
    "plt.ylabel('probability of k=1')\n",
    "plt.xlabel('for given different y values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use of the above things in measurement theory\n",
    "\n",
    "### Independent Observations\n",
    "\n",
    "Two observations are independent if the occurrence of one observation provides no information about the occurrence of the other observation. \n",
    "\n",
    "$$P(A \\cap B) = P(A)P(B)$$\n",
    "\n",
    "A simple instance is measuring the peak of absolutely everyone in your sample at a single factor of time. these are unrelated observations. however, if you had been to measure one infant’s height over a sure time period, these observations could be structured because the peak at every point of time could depend on the peak at the previous occasion. it is going to be\n",
    "barely extra than the preceding fee.\n",
    "\n",
    "### Covariance \n",
    "\n",
    "The measure of joint variability between two random variables is called covariance. This is with respect to probability theory and statistics.\n",
    "\n",
    "These variables shows similar behaviour when the greater values in one variable correspond to the greater values of another variable,this type of behaviour tells the covariance is positive.In the same way the variables shows opposite behaviour if the greater values in one variable correspond to the lesser values of another variable,this shows that the covariance is negative.Covariance tells linear relationship between variables.It's magnitude depends on the magnitude of the other variables because as the covariance is not normalized it is difficult to understand the magnitude of covariance.\n",
    "\n",
    "The covariance is denoted by $cov(X,Y)$ .here $X$ and $Y$ are random variables.\n",
    "Covariance is defined as  \n",
    "\n",
    "$$ cov(X,Y)=E[(X-E[X])(Y-E[Y]) $$\n",
    "\n",
    "If cov(X,Y)=0 then the two random variables $X$ and $Y$ are said to be uncorrelated.Generally Covariance is used to show the relationship between random variables.In terms of $X$ and $Y$,if the Values of $X-E[X]$ and $Y-E[Y]$ (which we get in a single experiment) have the same sign then we can say the covariance is positive and in the same way if the values of $X-E[X]$ and $Y-E[Y]$ are of opposite sign then we can say that the covaiance is negative.\n",
    "\n",
    "propagation of uncertainty..If the uncertainties are correlated then covariance should be taken into consideration. Correlation can stand up from two special resources. First reasource is that the measurement errors may be correlated. Second is that  when the underlying values are correlated across a populace, the uncertainties within the institution or group averages will be correlated....[11]\n",
    "\n",
    "### Correlation Coefficient\n",
    "The numerical measurement of the statistical relationship between two variables(Correlation) is known as correlation coefficient.\n",
    "\n",
    "There are many types of correlation but we generally use Pearson product-moment Correlation. The value of correlation coefficient varies between -1 to 1 .\n",
    "\n",
    "\n",
    "If the correlation coefficient of two variables X and Y are equal to 1 then they are said to be in perfect correlation.In the same way if the value of correlation coefficient is 0 then they is no correlation between the variables.If the Correlation Coefficient is -1 then the two variables are said to be in perfectly anticorrelation.\n",
    "\n",
    "The correlation coefficient of two random variables $X$,$Y$ which are having non zero variances is defined as\n",
    "\n",
    "$$\\rho (X,Y)= \\frac{cov(X,Y)}{\\sqrt{var(X)var(Y)}}$$\n",
    "\n",
    "Correlation coefficient is used to find the correlations in data which have uncertainity.\n",
    "\n",
    "### Bayesian Statistics\n",
    "\n",
    "Assume the information about the input quantity X includes a series of indications seemed as realization of independent, identically alloted random variables characterized by using a specific PDF, but with unknown expectation and variance.\n",
    "\n",
    "Bayes Theorem is used to find a PDF for X, where X is taken to be same as the unknown average of those random variables. It is carried out in two steps.\n",
    "In First step, non-informative joint prior (pre-data) PDF is assigned to the expectation and variance which is not known. Using Bayes Theorem, this joint prior PDF is then updated,this updation is achieved based on the information given by the series of indications, to get a joint posterior PDF for two unknown parameters. \n",
    "\n",
    "The desired posterior PDF for the average which is not known is then calculated as a marginal PDF by integrating over the possible values of the unknown variance.By using Bayes Theorem, the updating is done by finding the\n",
    "product of a likelihood function and the prior. The likelihood function in the case of indications obtained independently is the product of the functions,one function for each indication and indication to form example to Gaussian PDF. The\n",
    "posterior PDF is then calculated by integrating the product of prior PDF and likelihood over all possible values of the variance. Final answer is obtained after the normalizing resulting expression.\n",
    "\n",
    "If the indications are characterized by a PDF with only single parameter, a noninformative prior PDF is given to the unknown expectation of the random variables and the posterior distribution for X is given directly by Bayes Theorem,\n",
    "without the want of marginalization.....[1]\n",
    "\n",
    "\n",
    "### Multivariate normal distribution\n",
    "\n",
    "As univariate means only one variable, Multivariate means multiple variables.So Multivariate normal distribution means representing a normal distributions in multiple dimensions.for univariate normal distribution as it considers only one variable which shows only one dimension we have one mean but in case of multivariate we use multiple variables showing different dimensions.here different variables may be position, velocity and acceleration for aircraft or production of milk and feed rate at a dairy etc\n",
    "\n",
    "The multivariate normal distribution in regular is used to describe, any set of possibly correlated real-valued random variables each of which gathers around a mean value (at least approximately)....[12]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1] Dimitri P. Bertsekas and John N. Tsitsiklis *INTRODUCTION TO PROBABILITY SECOND EDITION*\n",
    "\n",
    "[2] S.V Gupta *MEASUREMENT UNCERTAINTITES physical parameters and calculation of uncertainities*\n",
    "\n",
    "[3] Probability slides of Bolic https://github.com/mbolic2/Uncertainty_Course/blob/master/Lec%203%20Probability%20Bayes%20Gaussian/Probability%20slides.pdf\n",
    "\n",
    "[4] Bolic, M. (2018). Principles of Data and Error Analysis in Engineering Measurements.\n",
    "\n",
    "[5]Multivariate gaussians from https://github.com/rlabbe/Kalman-and-Bayesian-Filters-in-Python/blob/master/05-Multivariate-Gaussians.ipynb\n",
    "\n",
    "[6]Properties of covariance from https://en.wikipedia.org/wiki/Covariance\n",
    "\n",
    "[7]Probability density function plot from https://en.wikipedia.org/wiki/Multivariate_normal_distribution\n",
    "\n",
    "[8]Example problem for independent events from https://www.enotes.com/homework-help/coin-tossed-single-6-sided-die-rolled-find-216923\n",
    "\n",
    "[9]Example problem for conditional probability from http://www.probabilityformula.org/conditional-probability-examples.html\n",
    "\n",
    "[10]Example problem for Bayes' theorem from https://github.com/mbolic2/Uncertainty_Course/blob/master/Lec%202%20Probability/Assignment1_allquestions.ipynb\n",
    "\n",
    "[11] Propagation of Uncertainity Wikipedia https://en.wikipedia.org/wiki/Propagation_of_uncertainty\n",
    "\n",
    "[12] Multivariate distribution https://en.wikipedia.org/wiki/Multivariate_normal_distribution"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
